% --- 1. CLASSE DU DOCUMENT ---
\documentclass[12pt, a4paper]{article} 

% --- 2. PACKAGES STANDARD ---
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{xcolor}

\usepackage[margin=2.5cm]{geometry}

% Double interligne (OBLIGATOIRE pour les reviewers)
\usepackage{setspace}
\doublespacing 

\usepackage{lineno}
\linenumbers

\usepackage[natbibapa]{apacite} % Style APA
\bibliographystyle{apacite}
\setlength{\bibsep}{10pt}

% --- DÉBUT DU DOCUMENT ---
\title{Study of the Influence of AI-Powered Virtual Agent Design on the Illusion of Understanding in Middle School Students} % Votre titre
\author{} % Anonyme pour le fichier principal
\date{}

\begin{document}

\maketitle


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/banner.png}
    \label{fig:teaser}
\end{figure}

% --- ABSTRACT STRUCTURÉ ---
\noindent\textbf{Abstract}\\
\textbf{Background:} The convergence of Large Language Models (LLMs) and synthetic media enables the creation of pedagogical agents with realistic human appearances. While anthropomorphic design is often assumed to enhance trust and engagement, its interaction with generative AI's linguistic fluency remains underexplored, particularly regarding metacognitive risks. Young learners may be particularly susceptible to these effects given their developmental tendency toward anthropomorphism.\\
\textbf{Objectives:} This study investigates how an AI agent's visual design (humanoid versus abstract) influences the ``Illusion of Understanding''---the tendency to overestimate one's knowledge---among adolescents. We examined whether humanoid appearance would elicit higher trust, greater perceived persuasion, and a stronger metacognitive illusion compared to an abstract visual representation.\\
\textbf{Methods:} In a between-groups experiment, 119 middle school students (ages 11--13) interacted with an AI agent delivering historical content about ancient Egyptian mummification. The agent was presented either as a hyper-realistic humanoid or as an abstract audio-reactive visualization, with identical voice and informational content across conditions. Measures included perceived anthropomorphism, multidimensional trust, perceived persuasion, self-assessed understanding at three time points, and objective knowledge tests.\\
\textbf{Results and Conclusions:} Contrary to predictions, the humanoid appearance elicited neither higher trust nor greater persuasion than the abstract condition. However, both conditions induced significant increases in self-assessed understanding without corresponding improvements in objective knowledge. These findings suggest that LLMs' conversational fluency may act as a predominant social cue, potentially fostering a metacognitive illusion of mastery independent of visual embodiment. Implications for the design of educational AI systems are discussed.

% --- LAY SUMMARY ---
\section*{Lay Summary}
\begin{itemize}
    \item We tested whether giving an AI tutor a realistic human face makes middle school students trust it more or learn better from it.
    \item 119 students (aged 11--13) interacted with an AI teaching assistant that looked either like a real person or like an abstract sound visualisation.
    \item Surprisingly, the human-like appearance did not make students trust the AI more or find it more convincing than the abstract version.
    \item Both versions made students feel more confident about their understanding, even though their actual knowledge did not improve---a phenomenon called the ``illusion of understanding.''
    \item For educators and AI designers: the smooth, confident way AI speaks may be more influential than how it looks. Future educational AI tools should be designed to reveal their limitations rather than appear perfect, helping students maintain critical thinking.
\end{itemize}

\clearpage

\noindent \textbf{Keywords:} Generative AI, Virtual Agents, Illusion of Understanding, Metacognition, History Education, Middle School.


% Content of the paper
\section{Introduction}
 % Text justification for a more formal appearance

The integration of generative artificial intelligence into educational environments opens new perspectives, particularly through the development of embodied conversational agents. Capable of generating adapted dialogues in real time, these technologies promise tailored pedagogical support \citep{yan2024}. However, their effectiveness depends not only on the relevance of their discourse, but also on the form this embodiment takes. The visual appearance of the agent constitutes a powerful lever that modulates the learner's perception, trust, and cognitive processes.
Research on human-machine interaction has long explored anthropomorphism, our tendency to project human traits onto non-human entities \citep{epley2007}. The "Computers as Social Actors" (CASA) paradigm has shown that social cues, even minimal ones, are sufficient to make us apply relational scripts to machines \citep{nass1994}, influencing initial trust \citep{li2023}.
However, earlier studies conducted well before the era of modern generative AI revealed that the pursuit of realism is not always an asset. By creating behavioral expectations, a human-like appearance makes any imperfection salient, potentially generating discomfort (Uncanny Valley) and degrading credibility \citep{nowak2004, mcdonnell2012}. Yet these works were conducted in a technological context that is now outdated. The advent of synthetic media, capable of generating agents with striking realism, has changed the equation. It is now possible to create photorealistic virtual instructors—"deepfakes"—with a fluidity and realism that blur the boundary between real and artificial \citep{whittaker2020, xu2025a}.
It is precisely this new capacity to embody LLMs that multiplies the stakes. On one hand, we have conversational "engines" capable of producing perfectly eloquent discourse; on the other, hyperrealistic visual "interfaces" to house them. The problem is that the linguistic fluency of LLMs masks an intrinsic flaw: their propensity for "hallucinations," that is, the generation of factually incorrect information presented with complete confidence \citep{zhang2025}. This fluidity maximizes "processing fluency," a cognitive bias that leads us to judge information that is easy to understand as being more true \citep{reber1999}. The risk is creating an "illusion of understanding"—the cognitive bias that leads individuals to overestimate their actual mastery of a subject \citep{rozenblit2002}, a particularly concerning phenomenon in a learning context. The central question, still little explored, is therefore: does giving a credible human face to LLM-generated discourse, by further activating social schemas, amplify this cognitive risk? Investigating this dynamic is particularly important for young learners, whose natural tendency toward anthropomorphism is more pronounced \citep{kidd2023}; consequently, they may be particularly sensitive to visual cues. The hyper-realistic appearance of generative agents could serve as a strong perceptual signal that reinforces this disposition, potentially facilitating the attribution of agency and competence \citep{Nguyen2022}.


The main objective of this study is to evaluate the influence of the visual design of a virtual agent (humanoid versus abstract), both animated by the same LLM, on middle school students (11-13 years old) in a history learning context. We hypothesized that the humanoid agent design would influence students' trust in the agent, perceived persuasiveness, and illusion of understanding due to increased perceived anthropomorphism and its effects on social perception. To test these predictions, we compared these subjective perceptions against objective learning outcomes in a controlled experimental setting. To contextualize our study, the following section offers a literature review on the illusion of understanding, the perception of virtual agents, and specific issues related to the use of LLMs in educational contexts. In Section 3, we then describe the experimental protocol. The results are presented in Section 4 and discussed in Section 5, where we address their theoretical and practical implications. Section 5 identifies the limitations of our work and future research directions. Finally, a conclusion in Section 6 summarizes the main contributions of this study.

\section{Literature Review}

\subsection{Agent Design and User Perception}

\subsubsection{Anthropomorphism and Social Perception}
Anthropomorphism refers to our tendency to attribute human characteristics, intentions, or emotions to non-human entities \citep{epley2007}. In human-machine interaction, visual appearance triggers this cognitive process. Users perceive a virtual agent first through its appearance, which then determines their attributions of personality, emotions, and trust \citep{mcdonnell2021}. Early research suggested a simple linear relationship: the more a computer representation resembles a human, the more it elicits social responses. \citet{gong2008} validated this continuum empirically, showing that increasing degrees of anthropomorphism, from a text interface to an actual human image, proportionally increased social judgments, social influence, and perceived competence. The CASA paradigm explains this mechanism: users unconsciously attribute human traits to computing agents and establish conversational norms with them in ways that mirror human-to-human interactions, even when these agents display minimal social cues \citep{nass1994}. \citet{nass2000} describe this phenomenon as “mindlessness,” characterizing how users automatically apply social scripts to machines, going so far as to attribute personality traits to them. This propensity extends to specific stereotypes, such as evaluating a computer according to gender bias based on its voice, for instance by perceiving a male voice as more competent on technical topics than a female voice delivering identical content \citep{nass1997}. Even when users consciously deny anthropomorphizing, these social scripts activate and encourage them to interact with the agent as they would with a human interlocutor \citep{kim2012}. These activated scripts increase perceived credibility. By fostering a sense of social presence \citep{biocca2003}, the impression of interacting with another social being, anthropomorphic cues enhance trust \citep{li2023}. Research on embodied conversational agents has been built on this premise, aiming for increasing realism to maximize engagement and persuasion \citep{guadagno2007}.

\subsubsection{Level of Realism and User Expectations}

Intuition suggests that a more realistic agent should be better perceived. Empirical research tends to demonstrate the opposite. Studies show a counterintuitive effect: agents with less human-like appearances are judged more credible and socially attractive than their more anthropomorphic equivalents \citep{nowak2004}. Minimalist computer interfaces exert greater influence on decision-making than human representations \citep{bengtsson1999}. A non-human character, such as a virtual cat, can be as persuasive as an actual speaker \citep{zanbaka2006}. These findings converge: there is a clear dissociation between the subjective perception of an agent (its resemblance to a human) and the behavioral trust users place in it (their willingness to follow its advice) \citep{kulms2019}. A highly realistic appearance constitutes an implicit promise of coherent human behavior, with all its complexity. When this promise is broken by slight imperfections, it tends to generate discomfort --- the ``Uncanny Valley'' \citep{mori2012, mcdonnell2012}. This fragility also concerns behavioral plausibility. Maximum behavioral realism is not always useful. \citep{groom2009} showed that consistency between an agent's appearance and behavior plays an important role: an unrealistic agent may be better perceived than a highly realistic but poorly animated agent, as it does not create expectations that are impossible to satisfy. Expectancy Violations Theory describes this mechanism: a highly realistic agent creates high expectations, and any deviation is judged negatively \citep{burgoon2015}. A recent study illustrates this counterintuitiveness: a simple smart speaker was perceived as more anthropomorphic than a humanoid robot, because it met the modest expectations it generated, while the robot disappointed the high expectations induced by its form \citep{haresamudram2024}. Managing these expectations becomes complicated when multiple communication channels combine. Alignment between modalities, such as appearance, voice, gestures, and emotional expressions, becomes a critical issue. The agent's competence in the task can take precedence over its appearance. A mismatch between a human appearance and a robotic voice may not affect trust if the agent provides accurate information and reliable in its guidance \citep{alimardani2024}. The context of interaction also determines user acceptance. Positive emotional expressiveness in a social context may seem inappropriate and harm trust in a critical task, where neutrality is expected \citep{torre2019}. Finally, there is a tension between engagement and informational goals. Rich animations and expressive gestures make the agent more liked, but these characteristics can visually distract the user and divert cognitive resources, undermining the persuasive effectiveness of the message \citep{parmar2022}.

\subsubsection{Voice Modalities and Visual Presence}

Beyond visual appearance, voice determines the perception of agents. The human voice serves as an indicator of the presence of a mind, capable of humanizing an interlocutor, while text-based communication can dehumanize \citep{schroeder2016}. The prosodic quality of the voice proves more important than visual embodiment for perception of naturalness. A realistic agent with inadequate prosody will be judged less natural than a disembodied agent with correct prosody \citep{ehret2021}. The ``voice effect'' predicted the superiority of human voice over synthetic voice for learning. Technological advances have challenged this assumption. Modern high-quality synthetic voices produce learning outcomes equivalent to, or even better than, those obtained with a human voice, particularly in terms of knowledge transfer \citep{craig2017}. Nevertheless, the human voice retains an advantage in establishing trust and perceived credibility, even if this advantage does not always translate into better learning performance \citep{craig2019, chiou2020}. The social presence induced by a human voice appears to mediate this increased credibility, an effect that operates independently of the agent's stated expertise \citep{kim2022}. The integration of a visual representation of the instructor --- its embodiment --- creates conflicting outcomes in the literature. The embodiment effect suggests that animated agents using gestures and facial expressions improve learning, by activating in the learner a sense of social partnership that promotes deeper cognitive processing \citep{mayer2012}. But substantial literature highlights the cognitive costs of this presence. The visual presence of an instructor, particularly an actual human, acts as an attentional magnet. Eye-tracking studies have demonstrated that learners spend considerable time looking at the instructor's face at the expense of the pedagogical content displayed on screen \citep{wang2017}. This visual distraction creates a striking dissociation between perception and performance: learners appreciate the lesson more, judge it more interesting, and have the impression of learning better, but their objective comprehension results decrease \citep{wilson2018}. This dynamic is summarized by the expression ``more gaze, but less learning.'' It is more pronounced with highly realistic instructors, who prove less effective than cartoon-like characters, as their social salience imposes a higher attentional cost \citep{li2024}.

\subsection{Generative AI and the Illusion of Understanding}

\subsubsection{Cognitive Biases in AI-Mediated Learning: Clarifying the Concepts}

Educational psychology has established that assessing one's own understanding is an intrinsically fallible process \citep{flavell1979}. This difficulty in self-assessment frequently leads learners to believe they have understood content even when they fail to detect obvious contradictions in it \citep{glenberg1982}. This phenomenon is particularly exacerbated with technology: \citet{salomon1984} demonstrated that when a medium is perceived as ``easy'' to process, learners reduce their mental investment (Amount of Invested Mental Effort), which degrades learning quality. The arrival of generative AI brings these mechanisms back to the forefront, but contemporary literature tends to conflate them under the generic term ``Illusion of Understanding.'' To ensure the precision of our analysis, this section draws a formal distinction between three concepts: the illusion of understanding as a metacognitive bias of self-assessment, epistemic overconfidence toward the source, and the illusory truth effect.

The first phenomenon, which we will refer to as \textbf{the Illusion of Understanding (IoU)}, is a metacognitive bias. It is defined as an individual's tendency to overestimate the depth of their own knowledge of a causal system until they are forced to produce a detailed explanation of it \citep{rozenblit2002}. In the field of multimedia learning, this bias has been formalized by the illusion of understanding model, which posits that certain presentation modalities, such as dynamic visual representations that passively illustrate a process, can make the material seem easier to process, lead learners to underestimate task difficulty and develop excessively optimistic metacomprehension \citep{paik2013}. The central mechanism of this illusion is \enquote{processing fluency}: information that is easy to perceive and process cognitively is experienced as familiar. This processing ease is then mistakenly attributed by the learner not to the intrinsic qualities of the presentation, but to their own mastery of the subject \citep{reber1999}. This results in a dissociation between subjective confidence, which increases, and objective learning, which stagnates or even decreases due to premature cognitive disengagement. This metacognitive attribution error manifests through several variants relevant to interactions with virtual agents. First, \enquote{instructor fluency} describes how nonverbal behaviors of the teacher—such as dynamism, eye contact, and verbal fluency—can bias students’ learning judgments \citep{toftness2018}. Learners confuse the quality of the pedagogical delivery with the quality of their own learning, reporting high confidence without corresponding performance gains. Second, the visual presence of an instructor can act as a \enquote{seductive detail}, increasing satisfaction and perceived learning while diverting attention from the content, which undermines deep comprehension \citep{wilson2018}. Finally, the passive observation of a motor task demonstration—such as watching the 'moonwalk' dance move or a tablecloth trick—can generate an  \enquote{illusion of skill acquisition}, where the observer mistakes the visual processing fluency (the fact that the action looks simple) for their own ability to execute it \citep{kardas2018}. These illusions are all the more pronounced as they fit within broader metacognitive deficits. The Dunning-Kruger effect highlights a \enquote{double burden}: the least competent individuals are not only those who perform worst, but they also lack the metacognitive skills necessary to recognize their own incompetence and calibrate their judgment \citep{kruger1999}. \textbf{In this article, we adopt this specific and metacognitive definition of the Illusion of Understanding, understood as a dissociation between subjective confidence in one's own abilities and measured objective knowledge.}

The second phenomenon, which should be distinguished from the first, is \textbf{overconfidence in source credibility}. This is not an error in self-assessment, but an epistemic error concerning external information. This bias manifests when learners place excessive trust in factually incorrect information because the source appears authoritative or the provided explanation is intuitively appealing. \citet{kulgemeyer2023} demonstrated that physics explanations based on misconceptions, because they align with everyday experience and common language, are often judged more comprehensible and convincing than scientifically correct but counterintuitive explanations. Although related, this epistemic risk is different from the illusion of mastery measured by our protocol. The implications of our results for this type of bias, particularly critical in the face of AI hallucinations, will be addressed in the Discussion section.

Finally, \textbf{the illusory truth effect} is a cognitive mechanism that primarily contributes to overconfidence in the source. It describes how the mere repetition of a statement, even if false, increases its perceived credibility by increasing its familiarity \citep{fazio2015}. This effect operates on the judgment of content veracity, reinforcing trust placed in the source that delivers it, but does not directly generate an illusion regarding the learner's own competence. It constitutes an explanatory variable for the persistence of false beliefs rather than a measure of learning self-assessment.

\subsubsection{Amplification of the Illusion of Understanding by Generative AI}

LLMs possess technical characteristics that risk specifically amplifying the metacognitive Illusion of Understanding. Their ability to generate instant, perfectly structured, and linguistically fluent discourse maximizes processing fluency for the user. When presented in a conversational rather than static text, AI-generated information is perceived as clearer, more engaging, and more credible, activating social heuristics that reduce critical vigilance \citep{anderl2024, huschens2023}. This apparent ease of interaction can induce what \citet{fernandes2026} call a \enquote{metacognitive paradox}: although AI assistance may improve immediate objective performance on a task, it simultaneously degrades the user's ability to assess that performance, leading to significant overestimation of their own competence and a disappearance of habitual metacognitive calibration. Similarly, users tend to overestimate the accuracy of AI models based on the apparent confidence of the explanations provided, a calibration gap that only reduces when the AI's style explicitly communicates  by using specific phrases to indicate its level of confidence (e.g.,'I am not sure' or 'I am confident') \citep{steyvers2025}. This reliance is exacerbated by misconceptions where users attribute human-like reasoning or even super-intelligence to the model \citep{ding2023, shanahan2024}. This dynamic favors a form of \enquote{metacognitive laziness}, where learners delegate regulation and evaluation processes to the machine, reducing their engagement in the germane cognitive activities necessary for deep cognitive processing \citep{fan2023, lee2025}. By making the learning task subjectively easier, the AI agent risks depriving students of \enquote{desirable difficulties}---the conscious effort of constructing, retrieving, and reorganizing knowledge---which are nevertheless essential for long-term retention \citep{bjork2013}. The reduction in mental effort observed during LLM use thus translates into a decline in conceptual learning quality \citep{stadler2024}. In this context, the increase in learner confidence does not signal increased mastery, but rather symptomatizes superficial information processing facilitated by the agent's fluency.

\subsubsection{Perception of Generative Educational Agents}

The integration of LLMs has led pedagogical agents to develop: from systems operating on the basis of predefined scripts and decision trees, they are now equipped with dialogue generation capabilities. This advance allows them to create personalised content in real time and adapt to the learner's needs, positioning them as promising tools for educational assistance \citep{pataranutaporn2021, yan2024}. However, the embodiment of this fluid discourse adds new considerations related to social perception to the risk of informational fallibility.
This evolution in conversational capabilities is amplified by parallel advances in synthetic media. Technologies such as generative adversarial networks (GANs) enable the creation of hyperrealistic visual representations of people, blurring the boundary between real and artificial and challenging the perception of authenticity \citep{whittaker2020}. 

Recent findings suggest that current generative technologies may effectively mitigate the uncanny valley effect, producing synthetic agents perceived as attractive \citep{xu2025a}. However, the pedagogical value of this realism appears to lie less in visual fidelity itself than in the relational expectations it establishes. Human-like appearance tends to act as a predictor of perceived utility primarily because it signals a higher potential for interactivity \citep{lin2024}. Consequently, visual design likely extends beyond mere aesthetics to function as a cue that shapes the learner's anticipation of a responsive exchange.



Research now focuses on the potential of this technological convergence for positive applications, seeking to move beyond the association of these tools with misinformation \citep{danry2022}. When integrated into visual representations, some studies suggest that these agents can achieve learning performance and subjective perception levels comparable to those of human instructors \citep{leiker2023, lim2024, xu2025a}.
To modulate learner perception, a documented strategy consists of exploiting the familiarity of the agent's appearance in order to activate preexisting social and relational schemas. The use of visual representations based on public personalities admired by the learner has proven to be a lever for improving motivation and perceived credibility \citep{pataranutaporn2022}. This effect is more pronounced when the connection is personal and no longer merely parasocial. A study comparing a text chatbot, a 3D mascot, and a "deepfake" of the students' actual teacher showed that preexisting familiarity with the teacher significantly increased trust and perceived competence of the AI agent \citep{tan2025}. However, this improvement in subjective perception  does not translate into objective learning gains. The agent's credibility is therefore not constructed ex nihilo, but can be transferred by capitalizing on already established social relationships, although this transfer appears to affect subjective engagement more than objective knowledge acquisition.


\subsubsection{Young Learners' Perception of AI}

Young learners, in particular, approach these technologies with cognitive and developmental predispositions that modulate their perception. Their tendency toward anthropomorphism, more pronounced than that of adults, leads them to more readily attribute intentions and mental states to technologies \citep{kidd2023}. This predisposition is documented in studies on home voice assistant use, where children frequently overestimate the intelligence of these systems. For example, even if a majority may judge it inappropriate to be rude to an assistant, a significant portion remains uncertain about its capacity to feel emotions or possess agency, suggesting they place these technologies in an intermediate ontological category, between object and living being \citep{andries2023}. This initial confusion makes them particularly sensitive to surface cues. Faced with fluent and well-structured discourse, they can thus judge information as reliable, regardless of its factual accuracy, simply because of the confidence its presentation inspires, a manifestation of the instructor fluency effect discussed earlier \citep{toftness2018}. However, this predisposition to trust is not absolute. Children are not passive recipients of information, but active evaluators who refine their judgment with experience. Direct confrontation with AI errors allows them to calibrate their trust and adopt a more critical stance \citep{solyst2024}. They are indeed accuracy-sensitive to responses, mobilizing their knowledge to reject incorrect information and even going so far as to "test" the agent with questions to which they know the answer \citep{oh2025, danovitch2013}. Their trust is also contextual, as they construct distinct mental models for each type of source. They intuitively understand the specific strengths of technology, for example by trusting a search engine more, perceived as access to massive information, than a human, perceived as relying on personal experience, to answer questions about the past \citep{girouard-hallam2025}.
This evaluation capacity relies on mental models that evolve rapidly with age and exposure to technologies. A 2022 study revealed a transition in early adolescence: 12-13 year-old students evaluated a chatbot according to its immediate utility, while 14-15 year-olds already compared it to the higher standards of consumer assistants \citep{Nguyen2022}. With the ubiquity of generative AI since then, it is plausible that this maturation of expectations now occurs earlier. These mental models are constructed from subtle cues. In the absence of physical embodiment, voice qualities become determining factors: an artificial voice, faster and prosodically less rich, leads children to attribute less agency to the speaker, even if the content it delivers is relevant \citep{xu2025}. The manner of speaking is therefore as powerful an indicator as message content for inferring the presence of a mind.
In sum, the literature establishes that the perceptible characteristics of an agent modulate learner trust in complex ways. However, much of this work, sometimes contradictory, was conducted with technologies whose visual realism and conversational capabilities were well below current standards \citep{bengtsson1999, nowak2004}. The question arises as to how these dynamics evolve when a hyperrealistic agent becomes the interface for fluent discourse generated by an LLM. The present study aims to address this gap by examining specifically how the visual design of an agent, comparing from a humanoid to an abstract representation, influences perceived anthropomorphism, trust in the agent, perceived persuasion, and ultimately the formation of the illusion of understanding in a learning context with young students.

\section{Materials and Methods}

\subsection{Experimental Conditions}

The study employed a between-groups experimental design with agent appearance as the independent variable. Each participating class was assigned to one of two experimental conditions.

Two distinct agent designs were created to manipulate the level of anthropomorphism while maintaining identical informational content and voice characteristics. Figure~\ref{fig:experimental_conditions} illustrates both experimental conditions.

\textbf{Humanoid Agent Condition}\\ In this condition (see Figure~\ref{fig:experimental_conditions}a), the virtual agent appeared as a middle-aged male professional speaking directly to the camera, with natural head movements and facial expressions synchronized with speech. This visual representation was generated using HeyGen\footnote{HeyGen: https://www.heygen.com}, an AI-based video generation platform, and featured a neutral office environment with formal attire. Facial expressions and movements were designed to appear natural yet contained, avoiding distracting theatricality.

\textbf{Abstract Agent Condition} \\ In this condition (see Figure~\ref{fig:experimental_conditions}b), the virtual agent was represented by a non-anthropomorphic audio-reactive visualization. The interface consisted of a central white pulsing circle whose size varied with audio amplitude, surrounded by 48 radial bars reacting to specific frequency bands of the voice spectrum.

% Figure with both experimental conditions side by side
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/human.jpg}
        \subcaption{Humanoid agent}
        \label{fig:humanoid_agent}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{images/abstract.png}
        \subcaption{Abstract agent}
        \label{fig:abstract_agent}
    \end{minipage}
    \caption{The two experimental conditions: (a) The humanoid agent as presented to participants with realistic human appearance and facial expressions, and (b) the abstract agent with its sound wave visualization.}
    \label{fig:experimental_conditions}
\end{figure}

\subsection{Software Implementation}

We developed Merlin, a custom application to present stimuli and manage the experimental session, using the Electron framework\footnote{Electron: https://www.electronjs.org} (v28.0.0). The application's architecture separates experimental control from stimulus presentation.


\subsubsection{Technical Architecture}

The system consists of three main components managed through Electron's multi-process model:

\begin{itemize}
    \item \textbf{Main Process:} Coordinates application lifecycle, window creation, and inter-process communication.
    
    \item \textbf{Operator Interface:} Allows the researcher to select experimental conditions, trigger agent responses, and control real-time parameters (audio volume, transition speed, fullscreen mode). The interface includes activity logging with timestamps and media preloading controls.
    
    \item \textbf{Participant Interface:} Displays the virtual agent on a screen visible to the class. This window was configured with consistent resolution and display settings across all experimental sessions to ensure standardized viewing conditions.
\end{itemize}

Figure~\ref{fig:system_overview} illustrates the complete experimental setup, showing both the participant-facing interfaces displaying the two agent conditions and the operator's control panel.


\subsubsection{Media Management and Synchronization}

For the humanoid agent condition, video sequences were created for different interaction states (idle animation, welcome message, responses to questions, closing message). The system implements double buffering for smooth transitions between video sequences, with CSS cross-fades ensuring seamless playback.

For the abstract agent condition, the audio-reactive visualization was implemented using HTML5 Canvas API and Web Audio API for real-time frequency analysis. The visual animation was synchronized with the audio responses through FFT (Fast Fourier Transform) analysis performed on the voice signal.

% Figure describing the overall system with both conditions
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{images/interface.png}
        \subcaption{Humanoid agent interface}
        \label{fig:interface_humanoid}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{images/interface_abstract.png}
        \subcaption{Abstract agent interface}
        \label{fig:interface_abstract}
    \end{minipage}
    \caption{Screenshot of the experimental software showing (a) the humanoid agent interface with the operator's control panel, and (b) the abstract agent interface with the same control panel.}
    \label{fig:system_overview}
\end{figure}


\subsubsection{Voice Synthesis and Content Standardization}

Both conditions used identical synthetic voice content generated by ElevenLabs\footnote{ElevenLabs: https://elevenlabs.io} with consistent parameters. The voice was selected for clarity, formal tone, and neutral characteristics, avoiding marked regional accents. Audio content for the abstract condition was extracted from the humanoid condition videos, ensuring equivalent informational content, speech rate, intonation, and prosody across conditions. This standardization ensures that observed differences between conditions reflect visual design rather than vocal variations.


\subsection{Participants}

One hundred and nineteen 7th-grade students ($N = 119$; $\text{mean age} = 11.77~\text{years}$, $SD = 0.49$) from a private state-contracted middle school participated in the study. The gender distribution was $76$ boys ($63.9\%$) and $43$ girls ($36.1\%$). Participants were recruited after obtaining informed consent from legal guardians and student assent. Participation in the research component (completing questionnaires and providing data for analysis) was voluntary. All students took part in  in the classroom activity, but only those who consented contributed their data to this study. Four classes were randomly assigned to one of the two experimental conditions: the ``humanoid agent'' condition ($n = 60$) or the ``abstract agent'' condition ($n = 59$).



\subsection{Procedure and Measures}

The experiment was conducted in a standard classroom equipped with a laptop running the Merlin application, a video projector for displaying the agent, and a sound system for broadcasting audio. A mobile external microphone was used by students designated as spokespersons during the interaction phase. The pedagogical content focused on ancient Egyptian mummification, a topic from the 6th-grade curriculum that participants had studied the previous year. The experiment took place in a single session of approximately 60 minutes per class, during regular school hours. Data anonymity was ensured by assigning a unique alphanumeric code to each student (e.g., A01, A02), reported on all collected documents.


\subsubsection{Pre-experiment Procedure}

The session began with a brief presentation on ancient Egyptian mummification. Basic demographic information (age and biological sex) was collected at this stage. Students then individually completed the first three phases of the adapted Illusion of Explanatory Depth (IOED) protocol \citep{rozenblit2002}. The target concept for all IOED measures was: \textit{"How did the ancient Egyptians go about transforming a dead person into a mummy?"}

The three initial phases were:

\begin{itemize}
    \item \textit{Phase 1 (Initial Self-Assessment [T1]):} Students rated their perceived knowledge level on a 7-point Likert scale ranging from 1 (\textit{"I know nothing at all"}) to 7 (\textit{"I know the subject very well"}).
    
    \item \textit{Phase 2 (Production of Explanations):} Students wrote an explanation of the mummification process in their own words within a limited space. They were instructed to explain the process as if teaching a friend or younger student unfamiliar with the topic, considering: the sequential steps, tools or materials used, duration of the process, its purpose, and the individuals responsible for performing it.
    
    \item \textit{Phase 3 (Intermediate Reassessment [T2]):} After producing their explanation, students re-assessed their knowledge level using the same 7-point scale.
\end{itemize}

These measures established a baseline of students' perceived understanding and explanatory ability prior to interacting with the agent.


\subsubsection{Experiment Procedure and Tasks}

Following the individual pre-test phase, students were organized into small working groups of 4 to 5 members. Each group received a list of six predefined questions addressing peripheral aspects of ancient Egyptian funerary practices and beliefs. These topics were intentionally distinct from the core mummification process assessed in the post-interaction knowledge test (detailed in Section 3.4.3). This approach prevented the agent from providing direct answers to the assessment, allowing us to measure if trust established during the interaction would generalize to the main subject: 

\begin{enumerate}
    \item "What were the most important gods for the Egyptians, and what was their role in protecting and guiding the dead in their journey to the afterlife?"
    \item "Beyond protecting the body, what was the symbolic and religious role of the sarcophagus in funerary rituals?"
    \item "Why was the death of a pharaoh such an important and spectacular event for the entire kingdom of Egypt?"
    \item "What were the everyday objects and food placed in tombs for, and what does this teach us about their vision of the afterlife?"
    \item "What was the function of the pyramids for the pharaoh, and for what reasons did the Egyptians later choose to build hidden tombs in the Valley of the Kings?"
    \item "What did the 'Opening of the Mouth' ritual consist of, and why was it considered essential for the mummy?"
\end{enumerate}

The agent's responses to these questions were generated using ChatGPT-5\footnote{ChatGPT-5, developed by OpenAI: https://openai.com/chatgpt} and subsequently validated by a history-geography teacher for historical accuracy and age-appropriate pedagogical content. These responses remained identical across both experimental conditions.

Groups were instructed to collectively discuss and select the two questions they considered most important or interesting. Within each group, one student was designated as spokesperson for the subsequent interaction phase. This collaborative selection phase lasted approximately 7 minutes.

The interaction phase with the agent (humanoid or abstract, depending on the class's experimental condition) then began. Participants were informed that the agent was powered by artificial intelligence. This disclosure was essential for both methodological and ethical reasons. Methodologically, our research question focuses on how the visual design of an AI interface influences user perception, not on whether an AI can deceive users into believing they are interacting with a human. Without this disclosure, several methodological issues would arise. First, participants would not activate their existing beliefs and attitudes toward AI systems, which are central to understanding how interface design modulates these pre-existing representations. Second, if participants had been led to believe they were interacting with a human expert, their responses would reflect judgments about "a human with an unusual appearance" rather than perceptions of an AI agent, thereby invalidating our core manipulation of agent design. Third, the deception would be fragile in a classroom setting, where a single student recognizing the artificial nature of the interaction could compromise the entire protocol. By ensuring transparency, we isolated the effect of visual design on human-AI interaction from confounding beliefs about the agent's nature. This transparency also fulfilled ethical requirements regarding informed participation. The agent was projected onto a central screen visible to all students.

Figure~\ref{fig:classroom_setup} shows the experimental setup during a typical interaction session.

% Figure of classroom setup
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\columnwidth]{images/IMG_5885.JPEG}
    \caption{Experimental setup in the classroom showing the agent projected on the screen during the interaction phase.}
    \label{fig:classroom_setup}
\end{figure}

During the interaction, designated spokespersons from each group took turns asking their selected question orally using a mobile microphone. The agent provided pre-programmed oral responses that were identical in content across both experimental conditions. This interaction phase lasted approximately 25 minutes, allowing each group to pose their question and hear the agent's response.


\subsubsection{Post-experiment Procedure}

Immediately after the interaction phase with the agent, students individually and silently completed the remaining measures in their paper booklet. This post-experimental phase assessed the evolution of their understanding and perceptions of the agent.

Students completed the final phase of the IOED protocol \citep{rozenblit2002} (Phase 4: Post-Interaction Final Reassessment [T3]), rating one last time their perceived knowledge of the same concept—\textit{"How did the ancient Egyptians go about transforming a dead person into a mummy?"}—on a 7-point Likert scale ranging from 1 (\textit{"I know nothing at all"}) to 7 (\textit{"I know the subject very well"}).


The booklet then contained a series of standardized scales to collect students' perceptions of the agent:
\begin{itemize}
    \item \textbf{Perceived Anthropomorphism of the Agent:} Measured using a French adaptation of the 'Anthropomorphism' subscale of the Godspeed Questionnaire Series \citep{bartneck2009}, comprising 5 items rated on 5-point semantic differential scales.
    \item \textbf{Multidimensional Trust:} Assessed via a French adaptation of the Multi-Dimensional Measure of Trust (MDMT) by \citet{malle2021}, composed of 16 items on 7-point Likert scales.
    \item \textbf{Perceived Persuasion:} Measured with a French adaptation of the scale by \citet{thomas2019}, consisting of 9 items on 7-point Likert scales.
\end{itemize}


Finally, a knowledge test co-designed with a history-geography teacher was administered to assess students' understanding of the mummification process. In contrast to the peripheral aspects of Egyptian funerary practices addressed by the agent's responses, this test focused on the core mummification procedure itself: the technical process, materials used, and underlying reasons for mummification. The test comprised three multiple-choice questions and two short-answer questions requiring written explanations of two to three sentences (see Appendix X for complete items). This design ensured the test measured actual understanding of mummification rather than simple retention of the agent's statements.


The completion of all these post-interaction questionnaires lasted approximately 20 minutes. During the final minutes of the session, students were informed of a forthcoming follow-up; a dedicated 30-to-40-minute debriefing session was subsequently conducted one week later in all participating classes to discuss Egyptian mummification, clarify information, and answer questions. In addition, the session would explain research goals in an age-appropriate manner, including the concept of illusion of understanding and critical thinking toward AI-based information sources.


\subsection{Hypotheses}

The convergence of generative technologies now makes it possible to combine hyperrealistic appearance with the conversational fluency of a LLM. This configuration invites us to re-examine the conclusions of previous work on anthropomorphism. Conducted in a different technological context, these studies observed that high realism could undermine credibility by creating unmet behavioural expectations \citep{bengtsson1999, nowak2004}. The question, then, is how an agent's visual design influences user perception when the conversational component has become fluid. Drawing on the CASA paradigm \citep{nass1994, nass2000}, we formulated the following hypotheses:

\begin{itemize}
    \item \textbf{H1: The humanoid agent will elicit higher trust than the abstract agent.}
    Human appearance is a powerful trigger of social scripts, automatic mechanisms that users unconsciously apply to interfaces \citep{nass2000}. This social predisposition is particularly marked among young adolescents, who evaluate a pedagogical agent more on its relational qualities, such as its sociability and the trust it inspires, than on technical performance criteria favored by older adolescents \citep{Nguyen2022}. Until recently, exploiting this lever was hindered by the visual imperfections of agents, creating a risk of rejection \citep{xu2025a}. However, technological advances now enable the generation of hyperrealistic agents whose subjective perception is comparable to that of human instructors \citep{lim2024}, and whose appeal is reinforced by association with familiar figures \citep{tan2025}. Consequently, we predict that the humanoid agent will be perceived as a more trustworthy source.
    However, this hypothesis is limited to subjective trust. The literature has indeed established a dissociation between the perception of an agent and behavioral trust, the latter being more influenced by the agent's perceived competence than by its appearance \citep{kulms2019, alimardani2024}.

    \item \textbf{H2: The humanoid agent will be perceived as more persuasive, leading students to accept its statements as true more readily.}
    Persuasion is partly determined by the credibility attributed to the source. The fluency and authoritative tone of discourse, characteristics of LLMs, tend to increase this perceived credibility and reduce critical vigilance, even among learners warned of the risk of error \citep{church2024, shekar2024}. We hypothesize that humanoid embodiment will amplify this effect. By activating social schemas, the humanoid agent should be perceived as a more credible source, which will lead students to accept its statements with less critical distance than those of the abstract agent. This prediction is formulated despite older work showing that less anthropomorphic agents could be more influential, suggesting that the capabilities of current technologies may have changed this dynamic \citep{bengtsson1999, zanbaka2006, gong2008}.

    \item \textbf{H3: The humanoid agent will induce a stronger illusion of understanding than the abstract agent.}
    The illusion of comprehension, which corresponds to an overestimation of one's own knowledge \citep{rozenblit2002}, can be triggered by processing heuristics. In both of our conditions, the discourse generated by the LLM, optimised for clarity \citep{shanahan2024}, creates a basic cognitive risk by promoting the fluency of the message: information that is easy to process is more likely to be judged as true, regardless of its content \citep{reber1999}. We postulate that the humanoid agent will amplify this risk by adding source fluency. This phenomenon describes how a presentation perceived as clear and confident can increase learners' confidence in their own mastery, regardless of their objective performance \citep{toftness2018}. The visual presence of a realistic instructor, acting as an ‘ seductive detail’, is known to maximise this effect by enhancing the learner’s subjective experience, sometimes to the detriment of learning \citep{wilson2018, li2024}. This amplification mechanism is particularly relevant for young learners, who initially display overconfidence in AI-generated content \citep{solyst2024} and whose mental models of technology, still in development, make them more dependent on surface cues to assess credibility \citep{andries2023}. We therefore predict that the humanoid agent will induce a significantly stronger illusion of understanding than the abstract agent.

\end{itemize}

\section{Results}

\subsection{Preliminary Analyses}

\subsubsection{Participant Demographics}

Demographic characteristics were examined across experimental conditions. The humanoid agent condition included $60$ participants ($\text{mean age} = 11.65~\text{years}$, $SD = 0.52$), while the abstract agent condition included $59$ participants ($\text{mean age} = 11.90~\text{years}$, $SD = 0.44$). Sex distribution was $40$ boys ($66.7\%$) and $20$ girls ($33.3\%$) in the humanoid condition, and $36$ boys ($61.0\%$) and $23$ girls ($39.0\%$) in the abstract condition. No participants were excluded based on the predefined criteria.

\subsubsection{Assumption Testing}

Prior to conducting parametric analyses, assumptions of normality and homogeneity of variances were evaluated. Levene's test for equality of variances indicated homogeneity of variances across conditions for \textbf{self-assessed understanding scores at all time points (T1: Initial, T2: Post-Explanation, T3: Post-Interaction)} ($F(1, 117) = 2.294$, $p = .133$; $F(1, 117) = 1.579$, $p = .211$; $F(1, 117) = 3.108$, $p = .080$), perceived anthropomorphism ($F(1, 117) = 0.108$, $p = .743$), and perceived persuasion ($F(1, 117) = 0.444$, $p = .507$). Heterogeneity of variances was observed for trust ($F(1, 117) = 4.357$, $p = .039$).

Shapiro-Wilk tests revealed significant deviations from normality for trust ($W = 0.949$, $p < .001$), explanation production score from Phase 2 ($W = 0.91$, $p < .001$), post-interaction knowledge test ($W = 0.97$, $p = .008$), and perceived persuasion ($W = 0.976$, $p = .032$). Perceived anthropomorphism demonstrated adequate normality ($W = 0.985$, $p = .206$). Consequently, non-parametric tests (Mann-Whitney U) were used for variables violating normality assumptions, while parametric tests (t-tests, ANOVAs) were used where assumptions were met.

For the repeated measures analyses of self-assessed understanding, Mauchly's test of sphericity was conducted. The sphericity assumption was satisfied ($\chi^2(2) = 5.52$, $p = .063$), indicating no violation of sphericity.

\subsection{Effects of Agent Design}

\subsubsection{Perceived Anthropomorphism (Manipulation Check)}

As shown in Figure~\ref{fig:descriptives_anthropomorphism}, which presents the comparison of perceived anthropomorphism between experimental conditions, an independent samples t-test examined differences. The humanoid agent ($M = 3.01, SD = 0.83$) was not perceived as significantly more anthropomorphic than the abstract agent ($M = 3.18, SD = 0.81$), $t(117) = 1.127, p = .262, d = 0.21, 95\% CI [-0.57, 0.15]$.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\columnwidth]{resources/8/_57_t-1436095900.png}
    \caption{Comparison of perceived anthropomorphism between experimental conditions.}
    \label{fig:descriptives_anthropomorphism}
\end{figure}


\subsubsection{Trust towards the agent (H1)}
Figure~\ref{fig:descriptives_trust} presents the comparison of trust between experimental conditions. Given the violation of homogeneity of variances and normality for this variable, a Mann-Whitney U test was conducted. The abstract agent ($M = 4.86, SD = 0.64$) was associated with marginally higher trust compared to the humanoid agent ($M = 4.59, SD = 0.83$), but indicated no significant difference, $U = 1502.50, p = .155, \text{rank-biserial correlation} = -.151$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\columnwidth]{resources/14/_34_t-1436463000.png}
    \caption{Comparison of trust between experimental conditions.}
    \label{fig:descriptives_trust}
\end{figure}

\subsubsection{Perceived Persuasion (H2)}
Figure~\ref{fig:descriptives_persuasion} illustrates the comparison of perceived persuasion levels. A Mann-Whitney U test was conducted to examine differences in perceived persuasion. No significant difference emerged between the humanoid agent ($M = 3.84, SD = 1.28$) and the abstract agent ($M = 3.63, SD = 1.20$), $U = 1965.50, p = .300, \text{rank-biserial correlation} = .11$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\columnwidth]{resources/16/_39_t-1436412247.png}
    \caption{Comparison of perceived persuasion between experimental conditions.}
    \label{fig:descriptives_persuasion}
\end{figure}




\subsubsection{Illusion of Explanatory Depth (H3)}
Figure~\ref{fig:ioed_time}  displays the evolution of self-assessed understanding across the three time points for both experimental conditions. A 2 (Condition: Humanoid, Abstract) $\times$ 3 (Time: T1, T2, T3) mixed repeated measures ANOVA was conducted to examine changes in self-assessed understanding across time points.  Repeated measures analyses revealed a significant main effect of Time, $F(2, 234) = 74.115, p < .001, \eta_p^2 = .39$, indicating that self-assessed understanding changed significantly across measurement points.  The interaction between Time and Condition was not significant, $F(2, 234) = 0.13, p = .878, \eta_p^2 < .001$. Consequently, the between-subjects effect of Condition was examined and found non-significant, $F(1, 117) = 0.025, p = .875, \eta_p^2 < .001$. Post-hoc pairwise comparisons with Holm correction for the main effect of Time revealed no significant difference between T1 (initial self-assessment) and T2 (post-explanation self-assessment), $t(117) = 0.446, p = .656, d = 0.03$. However, T3 (post-interaction self-assessment) was significantly higher than both T1, $t(117) = 9.374, p < .001, d = 0.69$, and T2, $t(117) = 11.572, p < .001, d = 0.72$. This pattern indicates that self-assessed understanding increased significantly after the interaction with the agent, regardless of agent design.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\columnwidth]{resources/3/_11_t-1437007136.png}
    \caption{Mean self-assessed understanding scores (IOED) across three time points (T1: initial assessment, T2: post-explanation assessment, T3: post-interaction assessment) for humanoid and abstract agent conditions. Error bars represent standard errors.}
    \label{fig:ioed_time}
\end{figure}


\subsubsection{Knowledge Measures}

\textbf{Explanation Production Score} \\
Figure~\ref{fig:descriptives_explanation} presents the comparison of explanation production scores between conditions. A Mann-Whitney U test revealed no significant difference in the quality of explanations produced during Phase 2 of the IOED protocol between the humanoid agent condition ($M = 5.92$, $SD = 4.81$) and the abstract agent condition ($M = 5.32$, $SD = 3.84$), $U = 1857.00$, $p = .644$, rank-biserial correlation = $.05$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\columnwidth]{resources/24/_48_t-1436192085.png}
    \caption{Comparison of explanation production scores between experimental conditions.}
    \label{fig:descriptives_explanation}
\end{figure}

\textbf{Post-Interaction Knowledge Test} \\
Figure~\ref{fig:descriptives_knowledge} presents the comparison of post-interaction knowledge test scores between conditions. A Mann-Whitney U test indicated no significant difference between the humanoid agent condition ($M = 2.90$, $SD = 1.71$) and the abstract agent condition ($M = 2.64$, $SD = 1.57$), $U = 1890.50$, $p = .517$, rank-biserial correlation = $.07$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\columnwidth]{resources/20/_43_t-1436304526.png}
    \caption{Comparison of post-interaction knowledge test scores between experimental conditions.}
    \label{fig:descriptives_knowledge}
\end{figure}
\section{Discussion}

The results of this study present a complex picture of the interaction between young learners and pedagogical agents whose discourse is generated by artificial intelligence. Contrary to most of our hypotheses, we observed no significant difference between the humanoid condition and the abstract condition regarding trust, perceived persuasion, or anthropomorphism. This null result, far from being an absence of finding, constitutes the starting point for a deeper analysis. It suggests that the integration of LLMs into conversational agents has reconfigured the hierarchy of social cues, where discourse fluency may eclipse the impact of visual embodiment. The following discussion aims to deconstruct this phenomenon by examining each hypothesis in turn in light of our results and the relevant literature.

\subsection{The Absence of Effect on Trust and Persuasion}

Contrary to our first hypothesis (H1), which predicted that the humanoid agent would elicit higher trust, the results revealed no significant difference between the two conditions on this measure (Figure \ref{fig:descriptives_trust}). Similarly, and contrary to our second hypothesis (H2), no significant difference was observed concerning perceived persuasion (Figure \ref{fig:descriptives_persuasion}). 

Equally unexpectedly, the humanoid agent was not perceived as significantly more anthropomorphic than the abstract agent (Figure \ref{fig:descriptives_anthropomorphism}). While this difference was  intended as a manipulation check rather than a formal hypothesis, its absence is a noteworthy outcome.  
Analysis of students' qualitative feedback suggests that this null result does not stem from an absence of effect of form, but rather from a balance of flaws: each agent failed to be perceived as superior, but for reasons specific to its nature.

The humanoid agent suffered from behavioral dissonance. The quality of its static visual representation was nonetheless a success, as students testified: "Without the character speaking or moving I would have thought it was a real person in a photo for example," to the point of sowing doubt about its artificial nature ("I had doubts, I thought it was a human"). This initial visual credibility established an implicit "coherence contract," creating very high expectations regarding its behavior. However, this contract was broken as soon as the agent became animated. Subtle imperfections — "it made the same gestures all the time," "only the mouth moves but not the rest of the face" — constituted a violation of these expectations that generated a feeling of strangeness \citep{groom2009, burgoon2015}. This phenomenon, where high visual realism not matched by equivalent behavioral realism degrades credibility, has been documented in previous studies \citep{bengtsson1999, nowak2004} and persists today \citep{haresamudram2024}. Although modern synthetic agents can avoid the uncanny valley effect \citep{xu2025a}, our stimulus clearly crossed a threshold of multimodal incoherence that canceled the expected social benefits of its embodiment.

Conversely, the abstract agent was penalized by "suspicious perfection." In the absence of a face to anchor evaluation, students focused exclusively on discourse characteristics. Their feedback, "Normally when we speak we have moments of reflection," is revealing. They mobilized an implicit heuristic of "cognitive work," associating intelligence with markers of reflection. The agent's perfect fluency was not interpreted as a sign of superior intelligence, but as proof of a simple prerecorded script ("It wasn't really an AI, it was a voice reciting a text"). This perfection signaled its artificiality and diminished its perceived agency. For young learners, paralinguistic cues are critical determinants for attributing agency \citep{xu2025}. The abstract agent's flawless performance thus paradoxically undermined its credibility as an intelligent "tutor." These two symmetrical failures were interpreted by students acting as active evaluators \citep{oh2025}. Informed of the AI's nature, and even recognizing the underlying technology ("It's ChatGPT!"), they judged the agents based on pre-existing schemas regarding AI capabilities rather than simple visual cues. This prior identification may have established a high baseline level of trust, but also greater sensitivity to flaws. Trust in technology is contextual; it depends on its past reliability \citep{danovitch2013} and its perceived adequacy for a task \citep{girouard-hallam2025}. In our case, in the absence of personal familiarity that could have oriented trust \citep{tan2025}, the two agents were judged functionally equivalent, confirming that adolescents’ expectations  mature faster than earlier literature suggested \citep{Nguyen2022}, they are no longer impressed by a machine's simple ability to speak, and they have become demanding judges of the perceived coherence and authenticity of virtual agents.

\subsection{The Illusion of Understanding}

Our third hypothesis (H3), which predicted that a humanoid agent would induce a stronger illusion of understanding, was not confirmed by the data. In contrast, the analysis reveals a more fundamental and uniform phenomenon that transcended our experimental conditions. In both groups, we observed a significant increase in self-assessed understanding between the moment when students attempted to explain the process themselves (T2) and the moment following their interaction with the agent (T3) (Figure \ref{fig:ioed_time}). The critical point is that this rise in confidence occurred while the agent provided no new information on the specific subject of the assessment — mummification. This gap between the content received and the confidence gained highlights the formation of a "confidence halo".
The Halo Effect—originally introduced by \citep{thorndike1920} and recently synthesized in the context of AI biases \citep{campbell2025}—describes a cognitive bias where 'one positive attribute of a person influences perceptions of their other unrelated traits.' In our context, results suggest that students may have generalized the agent's perceived competence on peripheral subjects (fluency, clarity on anecdotes) to the entire domain of knowledge."
A plausible explanation for this effect might lie not in the agent's appearance, but in the multimodal performance of its discourse, which seems to have been carried by two fluency factors acting in synergy. The first factor, of a linguistic nature, is the quality of content generated by the LLM. As students verbalized, the agent appeared "super clear and [...] very confident." This performance aligns with the theory of "instructor fluency," a bias where learners may mistakenly interpret a teacher's clarity as an indicator of their own learning \citep{toftness2018, wilson2018}. By maximizing "processing fluency" \citep{reber1999}, the discourse may have made information easy to assimilate, creating a strong impression of perceived competence.
The second factor, of an auditory nature, is the quality of the synthetic voice. Voice can indeed act as a vector of humanization \citep{schroeder2016}. Previous research has established that prosodic quality is often a more determining factor than visual appearance for perceived naturalness \citep{ehret2021}. However, our results highlight a tension in this dynamic. We propose that this interaction may be interpreted through the lens of Dual Process Theory \citep{kahneman2013}. While the prosodic fluency of the voice reduces cognitive load and likely fosters an immediate illusion of understanding (System 1 heuristic processing: "It is clear, so I understand"), its lack of natural disfluency (e.g., hesitations, breathing) may signal artificiality upon reflection (System 2 analytic processing: "It is too perfect to be human"), potentially triggering a sense of dissonance that hinders deeper relational trust.
The combination of these two fluencies — perfectly structured text and a prosodically convincing voice — likely created a performance perceived as so credible that it activated in students a social schema of expertise, evidenced by remarks comparing the agent to "a real History teacher" who "knew his subject very well."
This dual fluency, linguistic and auditory, could have consequences for cognitive engagement. It occurs within a conversational interaction, a presentation mode that tends to increase perceived credibility and reduce detection of inaccuracies compared to static text, as it can activate social heuristics that diminish critical vigilance \citep{anderl2024}. Even when content is not judged more credible overall, it is often perceived as clearer and more engaging, a quality that could lead to uncritical acceptance \citep{huschens2023}. This gap between subjective confidence and actual learning might not resemble a simple passive judgment error. It could be a consequence of the very nature of interaction with a generative agent, which could promote a form of "metacognitive laziness" \citep{fan2023}. Confronted with a source that provides instant and fluent responses, learners could be inclined to delegate the most costly cognitive processes. This tendency is supported by the "metacognitive paradox": although AI assistance can improve performance, it could simultaneously degrade the user's ability to evaluate that same performance, leading to overconfidence \citep{fernandes2026}. By making the learning task appear easier, the agent could deprive students of "desirable difficulties", the effort of constructing and reorganizing knowledge, which are nevertheless considered essential for lasting learning \citep{bjork2013}. In this perspective, the confidence regained at T3 could be interpreted not as the fruit of newly acquired knowledge, but as the symptom of externalized cognitive effort, of which the learner would not be fully aware.

\subsection{Knowledge Measures}
The results confirm the absence of significant difference between conditions on the quality of initial explanations (Figure \ref{fig:descriptives_explanation}) and post-interaction test scores (Figure \ref{fig:descriptives_knowledge}). This null result is consistent with the protocol: the agent provided no factual information that was assessed. However, this stability in objective performance contrasts with the increase in subjective confidence (IOED T3). Students feel more competent without having improved their knowledge: it is precisely this dissociation that characterizes the observed illusion of understanding. This phenomenon may be explained by the role of interaction itself. The agents --- whether humanoid (social distractors, \citep{li2024}) or abstract (potential perceptual distractors) --- do not simply divert attention.  They create a fluid interaction experience
that reduces the feeling of necessary cognitive effort. This
perceived ease promotes a passive metacognitive stance, in
line with seductive details theory \citep{wilson2018}, which posits that interesting but extraneous features in learning materials capture attention at the expense of processing the essential content, thereby creating an illusion of learning without actual comprehension.

\subsection{Theoretical and Practical Implications}

The results of this study have significant implications. On a theoretical level, they invite a reassessment of the CASA paradigm \citep{nass1994} and embodiment principle \citep{fiorella2021} in the age of generative agents. Conversational fluency emerges as a first-order social signal, capable of dominating the effects of visual appearance. The embodiment effect seems conditioned on a threshold of behavioral realism that current technologies struggle to achieve. More critically, our results demonstrate that this fluency creates a strong Illusion of Understanding, characterized by inflated self-assessment without objective gains \citep{paik2013}. While this metacognitive bias is problematic in itself, as it halts the learning process prematurely, its implications become alarming when contextualized within the inherent limitations of Generative AI. In our controlled protocol, the agent provided factually accurate information. However, in real-world scenarios, LLMs are prone to `hallucinations'---generating plausible but incorrect information presented with high confidence \citep{zhang2025, shanahan2024}.

This connects our findings on metacognitive illusion to the distinct phenomenon of epistemic overconfidence described by \citet{kulgemeyer2023}. They showed that learners readily accept scientifically flawed explanations when they are intuitive and presented fluently. Our study suggests a mechanism for this vulnerability: because the AI's fluency makes the learning task feel easy---what \citet{fernandes2026} describe as a `metacognitive paradox'---learners deactivate their critical vigilance. Consequently, they become susceptible to the `illusory truth effect' \citep{fazio2015}, where the sheer repetition and fluid presentation of a statement can override prior knowledge. If learners cannot distinguish between `feeling they understand' and `actually knowing' when the content is correct, as observed in our study, they are unlikely to detect the `negligent speech' of an hallucinating AI \citep{wachter2024}. Recent research confirms this danger, showing that deceptive AI explanations can be even more persuasive than honest ones \citep{danry2024} and that users place trust in medical AI responses despite low accuracy \citep{shekar2024}. Therefore, the `confidence halo' we observed---where students generalized the agent's perceived competence across the entire domain---acts as a credibility shield. On a practical level, these conclusions call for a change in philosophy in the design of pedagogical agents. The current trend aims to create ``transparent'' tools that fade away to leave room for the task. In Heideggerian terminology \citep{heidegger2016}, a generative agent is designed to be a perfectly ready-to-hand tool, a cognitive hammer that integrates so fluidly into action that it becomes invisible. Yet, it is precisely this invisibility that encourages metacognitive laziness. The tool only becomes an object of reflection, when it breaks or reveals a flaw. Our qualitative results show that students were precisely confronted with these ``ruptures'': the behavioral dissonance of the humanoid agent and the suspicious perfection of the abstract agent are the ``bugs'' that made the tool visible and forced a critical evaluation of its nature. The paradoxical implication is that, for pedagogical purposes, the objective should not be to create an even more perfect and invisible tool, but to script the rupture. Instead of maximizing trust, it must be calibrated by deliberately exposing the agent’s limitations. This means designing AI tutors that, far from providing direct and fluent answers, make their ``thinking'' process visible, admit their uncertainty, or force the learner to validate information. It involves transforming the agent from a ``black box'' into a ``discussion partner.'' Deliberately exposing learners to AI’s limitations has proven to be an effective strategy for moderating overconfidence and developing digital literacy adapted to tomorrow’s challenges \citep{solyst2024}.

\subsection{Limitations and Research Perspectives}

Although this study illuminates young learners' perception of generative agents, its conclusions must be considered in light of several limitations that define the scope of their validity and trace paths for future investigations. A fundamental limitation concerns the nature of interaction. To ensure rigorous experimental control, the agent's responses were pre-generated by an LLM and then validated, not produced in real time. Our study thus measured students' reaction to perfectly fluent and correct discourse, simulating an ideal agent, without exposing participants to the characteristics of direct interaction: latency, response variability, or risk of hallucinations \citep{zhang2025}. Future research will need to replicate this type of study with an interactive agent in real time, to observe how trust evolves when facing the actual performance of a stochastic system \citep{shanahan2024}. Second, the assigned task was highly structured, limited to asking predefined questions and listening passively. This framework does not reflect the complexity of real uses involving co-creation, problem-solving, or open-ended research. Studies show that even informed university students struggle to maintain critical vigilance during writing tasks \citep{church2024}. It would be relevant to explore these dynamics in project-based learning scenarios, where the agent becomes a tool to mobilize rather than a source to consume, to identify strategies for countering metacognitive laziness \citep{fan2023}. Third, our visual stimuli represent only a limited sample of possible designs. Results could differ by varying gender, perceived age, or ethnicity, known vectors of bias in human-machine interaction \citep{nass1997}. Future research could explore a continuum of anthropomorphism, test cartoon or non-human agents \citep{zanbaka2006}, and decouple the effects of voice and appearance, which were held constant in our study, to better understand their respective contribution to trust construction \citep{chiou2020}. Finally, our measures relied on self-reported scales that capture only subjective perception, not actual behavior. The literature establishes a dissociation between stated trust and behavioral trust \citep{kulms2019}. Future studies could integrate behavioral measures, where students make decisions with concrete consequences, to verify whether the observed "confidence halo" translates into behavioral overconfidence. These limitations outline a perspective centered on trust calibration. Building on our conclusion that perceptual "bugs" make the tool visible and elicit critical evaluation, future studies could test deliberately "imperfect" agents admitting their limitations \citep{soderlund2024} or introduce controlled errors. The empirical study of the effectiveness of "scripted ruptures" represents a promising research avenue. These could constitute an effective pedagogical remedy against the illusion of understanding and metacognitive laziness.


\section{Conclusion}
This study examined the influence of the visual design of a conversational agent powered by generative artificial intelligence on trust, persuasion, and the illusion of understanding among young adolescents. Contrary to initial predictions based on the CASA paradigm and social agency theory, the humanoid appearance did not elicit greater trust or engagement than the abstract agent. However, both conditions induced a significant increase in students' self-assessed understanding, without a corresponding improvement in their objective performance.
These results suggest a possible reconfiguration of the hierarchy of social cues in human-computer interaction in the era of large language models. Conversational fluency and the prosodic quality of speech—while potentially signaling artificiality upon reflection—function primarily as powerful cognitive heuristics that facilitate information processing, thereby overriding the effects of visual embodiment on metacognitive judgments. This apparent predominance of verbal fluency generates a major pedagogical risk: a metacognitive "illusion of mastery." By facilitating information processing, the AI agent, regardless of its appearance, seems able to lead learners to confuse the clarity of the presentation with their own competence, fostering cognitive passivity.
This dissociation between subjective confidence and actual knowledge is particularly critical given the propensity of generative models for hallucinations. If learners reduce their critical vigilance when facing correct content due to its fluency, as observed here, they become vulnerable to accepting erroneous information presented with the same confidence. Consequently, the challenge for the design of educational technologies no longer necessarily lies in the pursuit of maximal anthropomorphic realism, nor in the creation of "transparent" interfaces. On the contrary, to counter metacognitive laziness and calibrate trust, it appears necessary to design agents that make their limitations perceptible. Future research will need to explore the effectiveness of "scripted ruptures", design mechanisms that deliberately break fluency to re-engage the learner's monitoring processes—in order to attempt to transform the interaction with AI into a genuine critical partnership rather than passive information consumption.

\renewcommand{\bibfont}{\footnotesize}
\setlength{\bibsep}{2pt}

\bibliography{bibliography}

\end{document}