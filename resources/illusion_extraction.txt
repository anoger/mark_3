1 Introduction

The integration of generative artificial intelligence into ed-
ucational environments opens new perspectives, particu-
larly through the development of embodied conversational
agents. Capable of generating adapted dialogues in real
time, these technologies promise tailored pedagogical sup-
port [Yan et al., 2024]. However, their effectiveness de-
pends not only on the relevance of their discourse, but also
on the form this embodiment takes. The visual appearance
of the agent constitutes a powerful lever that modulates the
learner’s perception, trust, and cognitive processes. Re-
search on human-machine interaction has long explored an-
thropomorphism, our tendency to project human traits onto
non-human entities [Epley et al., 2007]. The "Computers
as Social Actors" (CASA) paradigm has shown that social
cues, even minimal ones, are sufficient to make us apply re-
lational scripts to machines [Nass et al., 1994], influencing
initial trust [Li et al., 2023]. However, pioneer studies con-
ducted well before the era of modern generative AI revealed
that the pursuit of realism is not always an asset. By creat-
ing behavioral expectations, a human-like appearance makes
any imperfection salient, potentially generating discomfort
(Uncanny Valley) and degrading credibility [Nowak, 2004,
McDonnell et al., 2012]. Yet these works were conducted in
a technological context that is now outdated. The advent of
synthetic media, capable of generating agents with striking
realism, has changed the equation. It is now possible to cre-
ate photorealistic virtual instructors—"deepfakes"—with a
fluidity and realism that blur the boundary between real and
artificial [Whittaker et al., 2020, Xu et al., 2025a]. It is
precisely this new capacity to embody LLMs that multiplies
the stakes. On one hand, we have conversational "engines"
capable of producing perfectly eloquent discourse; on the
other, hyperrealistic visual "interfaces" to house them. The
problem is that the linguistic fluency of LLMs masks an
intrinsic flaw: their propensity for "hallucinations," that is,
the generation of factually incorrect information presented
with complete confidence [Zhang et al., 2025]. This fluidity
maximizes "processing fluency," a cognitive bias that leads
us to judge information that is easy to understand as being
more true [Reber and Schwarz, 1999]. The risk is creat-
ing an "illusion of understanding"—the cognitive bias that
leads individuals to overestimate their actual mastery of a
subject [Rozenblit and Keil, 2002], a particularly concern-
ing phenomenon in a learning context. The central question,
still little explored, is therefore: does giving a credible hu-
man face to LLM-generated discourse, by further activating
social schemas, amplify this cognitive risk? Investigating
this dynamic is particularly important for young learners,
whose natural tendency toward anthropomorphism is more
pronounced [Kidd and Birhane, 2023], they may be particu-
larly sensitive to visual cues. The hyper-realistic appearance
of generative agents could serve as a strong perceptual sig-
nal that reinforces this disposition, potentially facilitating
the attribution of agency and competence [Nguyen, 2022].
The main objective of this study is to evaluate the influ-
ence of the visual design of a virtual agent (humanoid versus

──────────────────────────────────────────────────

abstract), both animated by the same LLM, on middle school

students (11-13 years old) in a history learning context. We
hypothesized that the humanoid agent design would influ-
ence students’ trust in the agent, perceived persuasiveness,
and illusion of understanding due to increased perceived
anthropomorphism and its effects on social perception. To
test these predictions, we compared these subjective per-
ceptions against objective learning outcomes in a controlled
experimental setting. To contextualize our study, the fol-
lowing section offers a literature review on the illusion of
understanding, the perception of virtual agents, and specific
issues related to the use of LLMs in educational contexts. In
Section 3, we then describe the experimental protocol. The
results are presented in Section 4 and discussed in Section
5, where we address their theoretical and practical implica-
tions. Section 5 identifies the limitations of our work and
future research directions. Finally, a conclusion in Section

──────────────────────────────────────────────────

6 summarizes the main contributions of this study.



──────────────────────────────────────────────────

2 Literature Review



──────────────────────────────────────────────────

2.1 Agent Design and User Perception



──────────────────────────────────────────────────

2.1.1 Anthropomorphism and Social Perception

Anthropomorphism refers to our tendency to attribute hu-
man characteristics, intentions, or emotions to non-human
entities [Epley et al., 2007]. In human-machine interaction,
visual appearance triggers this cognitive process. Users
perceive a virtual agent first through its appearance, which
then determines their attributions of personality, emotions,
and trust [McDonnell and Mutlu, 2021]. Early research
suggested a simple linear relationship: the more a com-
puter representation resembles a human, the more it elicits
social responses. Gong [2008] validated this continuum em-
pirically, showing that increasing degrees of anthropomor-
phism, from a text interface to an actual human image, pro-
portionally increased social judgments, social influence, and
perceived competence. The CASA paradigm explains this
mechanism: users unconsciously attribute human traits to
computing agents and establish conversational norms with
them in ways that mirror human-to-human interactions, even
when these agents display minimal social cues [Nass et al.,
1994]. Nass and Moon [2000] describe this phenomenon
as “mindlessness,” characterizing how users automatically
apply social scripts to machines, going so far as to attribute
personality traits to them. This propensity extends to spe-
cific stereotypes, such as evaluating a computer according
to gender bias based on its voice, for instance by perceiv-
ing a male voice as more competent on technical topics
than a female voice delivering identical content [Nass et al.,
1997]. Even when users consciously deny anthropomor-
phizing, these social scripts activate and encourage them
to interact with the agent as they would with a human in-
terlocutor [Kim and Sundar, 2012]. These activated scripts
increase perceived credibility. By fostering a sense of social
presence [Biocca et al., 2003], the impression of interacting
with another social being, anthropomorphic cues enhance
trust [Li et al., 2023]. Research on embodied conversational
agents has been built on this premise, aiming for increasing
realism to maximize engagement and persuasion [Guadagno
et al., 2007].

──────────────────────────────────────────────────

2.1.2 Level of Realism and User Expectations

Intuition suggests that a more realistic agent should be better
perceived. Empirical research tends to demonstrate the op-
posite. Studies show a counterintuitive effect: agents with
less human-like appearances are judged more credible and
socially attractive than their more anthropomorphic equiva-
lents [Nowak, 2004]. Minimalist computer interfaces exert
greater influence on decision-making than human represen-
tations [Bengtsson et al., 1999]. A non-human character,
such as a virtual cat, can be as persuasive as an actual
speaker [Zanbaka et al., 2006]. These findings converge:
there is a clear dissociation between the subjective percep-
tion of an agent (its resemblance to a human) and the be-
havioral trust users place in it (their willingness to follow
its advice) [Kulms and Kopp, 2019]. A highly realistic ap-
pearance constitutes an implicit promise of coherent human
behavior, with all its complexity. When this promise is bro-
ken by slight imperfections, it tends to generate discomfort
— the “Uncanny Valley” [Mori et al., 2012, McDonnell
et al., 2012]. This fragility also concerns behavioral plau-
sibility. Maximum behavioral realism is not always useful.
[Groom et al., 2009] showed that consistency between an
agent’s appearance and behavior plays an important role:
an unrealistic agent may be better perceived than a highly
realistic but poorly animated agent, as it does not create
expectations that are impossible to satisfy. Expectancy Vi-
olations Theory describes this mechanism: a highly re-
alistic agent creates high expectations, and any deviation
is judged negatively [Burgoon, 2015]. A recent study il-
lustrates this counterintuitiveness: a simple smart speaker
was perceived as more anthropomorphic than a humanoid
robot, because it met the modest expectations it generated,
while the robot disappointed the high expectations induced
by its form [Haresamudram et al., 2024]. Managing these
expectations becomes complicated when multiple commu-
nication channels combine. Alignment between modalities,
such as appearance, voice, gestures, and emotional expres-
sions, becomes a critical issue. The agent’s competence in
the task can take precedence over its appearance. A mis-
match between a human appearance and a robotic voice
may not affect trust if the agent provides accurate informa-
tion and reliable in its guidance [Alimardani et al., 2024].
The context of interaction also determines user acceptance.
Positive emotional expressiveness in a social context may
seem inappropriate and harm trust in a critical task, where
neutrality is expected [Torre et al., 2019]. Finally, there is a
tension between engagement and informational goals. Rich
animations and expressive gestures make the agent more
liked, but these characteristics can visually distract the user
and divert cognitive resources, undermining the persuasive
effectiveness of the message [Parmar et al., 2022].

──────────────────────────────────────────────────

2.1.3 Voice Modalities and Visual Presence

Beyond visual appearance, voice determines the percep-
tion of agents. The human voice serves as an indicator of
the presence of a mind, capable of humanizing an inter-
locutor, while text-based communication can dehumanize
[Schroeder and Epley, 2016]. The prosodic quality of the
voice proves more important than visual embodiment for
perception of naturalness. A realistic agent with inade-
quate prosody will be judged less natural than a disembod-
ied agent with correct prosody [Ehret et al., 2021]. The
“voice effect” predicted the superiority of human voice over
synthetic voice for learning. Technological advances have
challenged this assumption. Modern high-quality synthetic
voices produce learning outcomes equivalent to, or even
better than, those obtained with a human voice, particu-
larly in terms of knowledge transfer [Craig and Schroeder,
2017]. Nevertheless, the human voice retains an advan-
tage in establishing trust and perceived credibility, even if
this advantage does not always translate into better learn-
ing performance [Craig et al., 2019, Chiou et al., 2020].
The social presence induced by a human voice appears to
mediate this increased credibility, an effect that operates
independently of the agent’s stated expertise [Kim et al.,
2022]. The integration of a visual representation of the in-
structor — its embodiment — creates conflicting outcomes
in the literature. The embodiment effect suggests that ani-
mated agents using gestures and facial expressions improve
learning, by activating in the learner a sense of social part-
nership that promotes deeper cognitive processing [Mayer
and DaPra, 2012]. But substantial literature highlights the
cognitive costs of this presence. The visual presence of an
instructor, particularly an actual human, acts as an atten-
tional magnet. Eye-tracking studies have demonstrated that
learners spend considerable time looking at the instructor’s
face at the expense of the pedagogical content displayed on
screen [Wang and Antonenko, 2017]. This visual distrac-
tion creates a striking dissociation between perception and
performance: learners appreciate the lesson more, judge it
more interesting, and have the impression of learning better,
but their objective comprehension results decrease [Wilson
et al., 2018]. This dynamic is summarized by the expression
“more gaze, but less learning.” It is more pronounced with
highly realistic instructors, who prove less effective than
cartoon-like characters, as their social salience imposes a
higher attentional cost [Li et al., 2024].

──────────────────────────────────────────────────

2.2 Generative AI and the Illusion of Understand-

ing

──────────────────────────────────────────────────

2.2.1 Cognitive Biases in AI-Mediated Learning:

Clarifying the Concepts
Educational psychology has established that assessing
one’s own understanding is an intrinsically fallible process
[Flavell, 1979]. This difficulty in self-assessment frequently
leads learners to believe they have understood content even
when they fail to detect obvious contradictions in it [Glen-
berg et al., 1982]. This phenomenon is particularly exacer-
bated with technology: Salomon [1984] demonstrated that
when a medium is perceived as “easy” to process, learners
reduce their mental investment (Amount of Invested Mental
Effort), which degrades learning quality. The arrival of gen-
erative AI brings these mechanisms back to the forefront,
but contemporary literature tends to conflate them under
the generic term “Illusion of Understanding.” To ensure the
precision of our analysis, this section draws a formal distinc-
tion between three concepts: the illusion of understanding
as a metacognitive bias of self-assessment, epistemic over-
confidence toward the source, and the illusory truth effect.
The first phenomenon, which we will refer to as the Il-
lusion of Understanding (IoU), is a metacognitive bias. It
is defined as an individual’s tendency to overestimate the
depth of their own knowledge of a causal system until they
are forced to produce a detailed explanation of it [Rozenblit
and Keil, 2002]. In the field of multimedia learning, this
bias has been formalized by the illusion of understanding
model, which posits that certain presentation modalities,
such as dynamic visual representations that passively il-
lustrate a process, can make the material seem easier to
process, lead learners to underestimate task difficulty and
develop excessively optimistic metacomprehension [Paik
and Schraw, 2013]. The central mechanism of this illu-
sion is “processing fluency”: information that is easy to
perceive and process cognitively is experienced as familiar.
This processing ease is then mistakenly attributed by the
learner not to the intrinsic qualities of the presentation, but
to their own mastery of the subject [Reber and Schwarz,
1999]. This results in a dissociation between subjective
confidence, which increases, and objective learning, which
stagnates or even decreases due to premature cognitive dis-
engagement. This metacognitive attribution error manifests
through several variants relevant to interactions with virtual
agents. First, “instructor fluency” describes how nonverbal
behaviors of the teacher—such as dynamism, eye contact,
and verbal fluency—can bias students’ learning judgments
[Toftness et al., 2018]. Learners confuse the quality of the
pedagogical delivery with the quality of their own learn-
ing, reporting high confidence without corresponding per-
formance gains. Second, the visual presence of an instruc-
tor can act as a “seductive detail”, increasing satisfaction
and perceived learning while diverting attention from the
content, which undermines deep comprehension [Wilson
et al., 2018]. Finally, the passive observation of a mo-
tor task demonstration—such as watching the ’moonwalk’
dance move or a tablecloth trick—can generate an “illusion
of skill acquisition”, where the observer mistakes the visual
processing fluency (the fact that the action looks simple) for
their own ability to execute it [Kardas and O’Brien, 2018].
These illusions are all the more pronounced as they fit within
broader metacognitive deficits. The Dunning-Kruger effect
highlights a “double burden”: the least competent individ-
uals are not only those who perform worst, but they also
lack the metacognitive skills necessary to recognize their
own incompetence and calibrate their judgment [Kruger
and Dunning, 1999]. In this article, we adopt this spe-
cific and metacognitive definition of the Illusion of Un-
derstanding, understood as a dissociation between sub-
jective confidence in one’s own abilities and measured
objective knowledge.
The second phenomenon, which should be distinguished
from the first, is overconfidence in source credibility. This
is not an error in self-assessment, but an epistemic error
concerning external information. This bias manifests when
learners place excessive trust in factually incorrect informa-
tion because the source appears authoritative or the provided
explanation is intuitively appealing. Kulgemeyer and Wit-
twer [2023] demonstrated that physics explanations based
on misconceptions, because they align with everyday expe-
rience and common language, are often judged more com-
prehensible and convincing than scientifically correct but
counterintuitive explanations. Although related, this epis-
temic risk is different from the illusion of mastery measured
by our protocol. The implications of our results for this type
of bias, particularly critical in the face of AI hallucinations,
will be addressed in the Discussion section.
Finally, the illusory truth effect is a cognitive mech-
anism that primarily contributes to overconfidence in the
source. It describes how the mere repetition of a statement,
even if false, increases its perceived credibility by increas-
ing its familiarity [Fazio et al., 2015]. This effect operates
on the judgment of content veracity, reinforcing trust placed
in the source that delivers it, but does not directly generate
an illusion regarding the learner’s own competence. It con-
stitutes an explanatory variable for the persistence of false
beliefs rather than a measure of learning self-assessment.

──────────────────────────────────────────────────

2.2.2 Amplification of the Illusion of Understanding

by Generative AI
LLMs possess technical characteristics that risk specifically
amplifying the metacognitive Illusion of Understanding.
Their ability to generate instant, perfectly structured, and
linguistically fluent discourse maximizes processing fluency
for the user. When presented in a conversational rather
than static text, AI-generated information is perceived as
clearer, more engaging, and more credible, activating social
heuristics that reduce critical vigilance [Anderl et al., 2024,
Huschens et al., 2023]. This apparent ease of interaction can
induce what Fernandes et al. [2026] call a “metacognitive
paradox”: although AI assistance may improve immediate
objective performance on a task, it simultaneously degrades
the user’s ability to assess that performance, leading to sig-
nificant overestimation of their own competence and a dis-
appearance of habitual metacognitive calibration. Similarly,
users tend to overestimate the accuracy of AI models based
on the apparent confidence of the explanations provided, a
calibration gap that only reduces when the AI’s style ex-
plicitly communicates by using specific phrases to indicate
its level of confidence (e.g.,’I am not sure’ or ’I am confi-
dent’) [Steyvers et al., 2025]. This reliance is exacerbated by
misconceptions where users attribute human-like reasoning
or even super-intelligence to the model [Ding et al., 2023,
Shanahan, 2024]. This dynamic favors a form of “metacog-
nitive laziness”, where learners delegate regulation and eval-
uation processes to the machine, reducing their engagement
in the germane cognitive activities necessary for deep cog-
nitive processing [Fan et al., 2023, Lee et al., 2025]. By
making the learning task subjectively easier, the AI agent
risks depriving students of “desirable difficulties”—the con-
scious effort of constructing, retrieving, and reorganizing
knowledge—which are nevertheless essential for long-term
retention [Bjork et al., 2013]. The reduction in mental effort
observed during LLM use thus translates into a decline in
conceptual learning quality [Stadler et al., 2024]. In this
context, the increase in learner confidence does not signal
increased mastery, but rather symptomatizes superficial in-
formation processing facilitated by the agent’s fluency.

──────────────────────────────────────────────────

2.2.3 Perception of Generative Educational Agents

The integration of LLMs has led pedagogical agents to de-
velop: from systems operating on the basis of predefined
scripts and decision trees, they are now equipped with di-
alogue generation capabilities. This advance allows them
to create personalised content in real time and adapt to the
learner’s needs, positioning them as promising tools for ed-
ucational assistance [Pataranutaporn et al., 2021, Yan et al.,
2024]. However, the embodiment of this fluid discourse
adds new considerations related to social perception to the
risk of informational fallibility. This evolution in conver-
sational capabilities is amplified by parallel advances in
synthetic media. Technologies such as generative adversar-
ial networks (GANs) enable the creation of hyperrealistic
visual representations of people, blurring the boundary be-
tween real and artificial and challenging the perception of
authenticity [Whittaker et al., 2020].
Recent findings suggest that current generative technolo-
gies may effectively mitigate the uncanny valley effect, pro-
ducing synthetic agents perceived as attractive [Xu et al.,
2025a]. However, the pedagogical value of this realism ap-
pears to lie less in visual fidelity itself than in the relational
expectations it establishes. Human-like appearance tends
to act as a predictor of perceived utility primarily because
it signals a higher potential for interactivity [Lin and Yu,
2024]. Consequently, visual design likely extends beyond
mere aesthetics to function as a cue that shapes the learner’s
anticipation of a responsive exchange.
Research now focuses on the potential of this technolog-
ical convergence for positive applications, seeking to move
beyond the association of these tools with misinformation
[Danry et al., 2022]. When integrated into visual represen-
tations, some studies suggest that these agents can achieve
learning performance and subjective perception levels com-
parable to those of human instructors [Leiker et al., 2023,
Lim, 2024, Xu et al., 2025a]. To modulate learner per-
ception, a documented strategy consists of exploiting the
familiarity of the agent’s appearance in order to activate
preexisting social and relational schemas. The use of visual
representations based on public personalities admired by the
learner has proven to be a lever for improving motivation
and perceived credibility [Pataranutaporn et al., 2022]. This
effect is more pronounced when the connection is personal
and no longer merely parasocial. A study comparing a text
chatbot, a 3D mascot, and a "deepfake" of the students’
actual teacher showed that preexisting familiarity with the
teacher significantly increased trust and perceived compe-
tence of the AI agent [Tan et al., 2025]. However, this im-
provement in subjective perception does not translate into
objective learning gains. The agent’s credibility is therefore
not constructed ex nihilo, but can be transferred by capital-
izing on already established social relationships, although
this transfer appears to affect subjective engagement more
than objective knowledge acquisition.

──────────────────────────────────────────────────

2.2.4 Young Learners’ Perception of AI

Young learners, in particular, approach these technologies
with cognitive and developmental predispositions that mod-
ulate their perception. Their tendency toward anthropomor-
phism, more pronounced than that of adults, leads them to
more readily attribute intentions and mental states to tech-
nologies [Kidd and Birhane, 2023]. This predisposition is
documented in studies on home voice assistant use, where
children frequently overestimate the intelligence of these
systems. For example, even if a majority may judge it
inappropriate to be rude to an assistant, a significant por-
tion remains uncertain about its capacity to feel emotions
or possess agency, suggesting they place these technolo-
gies in an intermediate ontological category, between ob-
ject and living being [Andries and Robertson, 2023]. This
initial confusion makes them particularly sensitive to sur-
face cues. Faced with fluent and well-structured discourse,
they can thus judge information as reliable, regardless of its
factual accuracy, simply because of the confidence its pre-
sentation inspires, a manifestation of the instructor fluency
effect discussed earlier [Toftness et al., 2018]. However,
this predisposition to trust is not absolute. Children are not
passive recipients of information, but active evaluators who
refine their judgment with experience. Direct confronta-
tion with AI errors allows them to calibrate their trust and
adopt a more critical stance [Solyst et al., 2024]. They
are indeed accuracy-sensitive to responses, mobilizing their
knowledge to reject incorrect information and even going
so far as to "test" the agent with questions to which they
know the answer [Oh et al., 2025, Danovitch and Alzahabi,
2013]. Their trust is also contextual, as they construct dis-
tinct mental models for each type of source. They intuitively
understand the specific strengths of technology, for exam-
ple by trusting a search engine more, perceived as access
to massive information, than a human, perceived as relying
on personal experience, to answer questions about the past
[Girouard-Hallam and Danovitch, 2025]. This evaluation
capacity relies on mental models that evolve rapidly with
age and exposure to technologies. A 2022 study revealed
a transition in early adolescence: 12-13 year-old students
evaluated a chatbot according to its immediate utility, while
14-15 year-olds already compared it to the higher standards
of consumer assistants [Nguyen, 2022]. With the ubiquity of
generative AI since then, it is plausible that this maturation
of expectations now occurs earlier. These mental models
are constructed from subtle cues. In the absence of physical
embodiment, voice qualities become determining factors:
an artificial voice, faster and prosodically less rich, leads
children to attribute less agency to the speaker, even if the
content it delivers is relevant [Xu et al., 2025b]. The manner
of speaking is therefore as powerful an indicator as message
content for inferring the presence of a mind. In sum, the
literature establishes that the perceptible characteristics of
an agent modulate learner trust in complex ways. However,
much of this work, sometimes contradictory, was conducted
with technologies whose visual realism and conversational
capabilities were well below current standards [Bengtsson
et al., 1999, Nowak, 2004]. The question arises as to how
these dynamics evolve when a hyperrealistic agent becomes
the interface for fluent discourse generated by an LLM. The
present study aims to address this gap by examining specif-
ically how the visual design of an agent, comparing from a
humanoid to an abstract representation, influences perceived
anthropomorphism, trust in the agent, perceived persuasion,
and ultimately the formation of the illusion of understanding
in a learning context with young students.