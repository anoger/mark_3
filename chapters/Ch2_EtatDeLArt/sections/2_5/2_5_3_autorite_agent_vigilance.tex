\subsection{L'autorité de l'agent et la vigilance épistémique}
\label{subsec:autorite_agent_vigilance}

Les mécanismes décrits dans les deux sections précédentes --- déficit métacognitif et heuristique de fluidité --- opèrent indépendamment de la forme de l'agent. Un texte fluide suffit à les activer. L'incarnation visuelle de l'agent ajoute un troisième facteur : les indices sociaux qui modulent la crédibilité perçue et la vigilance critique de l'apprenant.

Les mécanismes d'anthropomorphisme et de traitement social automatique décrits en~\ref{subsec:presence_sociale_CASA} prennent ici une dimension spécifique. Un agent pédagogique hyperréaliste combine trois registres de signaux. Le discours fluide active l'heuristique de fluidité (cf.~\ref{subsec:heuristique_fluidite}). Le visage humain et les expressions faciales activent les scripts sociaux du paradigme CASA. La voix naturelle ajoute un indice de présence d'esprit (\textit{mind perception}). Chacun de ces registres active des heuristiques de crédibilité distinctes (cf.~\ref{subsec:incarnation_agence_sociale}). Leur convergence tend à produire un signal de compétence perçue dont l'intensité peut être largement indépendante de la compétence réelle de l'agent. L'apprenant ne distingue pas entre la qualité de l'interface et la qualité du contenu.

L'assistance par IA modifie la relation de l'apprenant à sa propre évaluation. L'IA tend à améliorer la performance immédiate sur une tâche, mais peut simultanément dégrader la capacité de l'utilisateur à évaluer cette performance \citep{fernandes2026}. La calibration métacognitive habituelle --- l'ajustement de la confiance en fonction de la difficulté perçue --- semble s'affaiblir considérablement. L'utilisateur ne sait plus ce qu'il sait, parce que l'aisance de l'interaction lui donne l'impression de comprendre. Ce phénomène a été qualifié de \textit{metacognitive laziness} : l'apprenant délègue les processus de régulation et d'évaluation à la machine, réduisant son engagement dans les activités cognitives nécessaires au traitement profond \citep{fan2023, lee2025}.

Cette délégation a un coût. L'apprentissage profond requiert un effort cognitif que la facilité de l'IA tend à supprimer. Les \textit{desirable difficulties} --- l'effort conscient de construction, de récupération et de réorganisation des connaissances --- sont des conditions de la rétention à long terme \citep{bjork2013}. En rendant la tâche subjectivement plus facile, l'agent risque de priver l'apprenant de cet effort. La réduction de l'effort mental observée lors de l'utilisation de LLM semble se traduire par un déclin de la qualité de l'apprentissage conceptuel \citep{stadler2024}. La confiance accrue de l'apprenant ne signale pas nécessairement une maîtrise accrue. Elle peut signaler un traitement superficiel facilité par la fluence de l'agent.

Les jeunes apprenants présentent une vulnérabilité spécifique à cette dynamique. Leur tendance à l'anthropomorphisme est plus prononcée que celle des adultes \citep{kidd2023}. Confrontés aux assistants vocaux domestiques, les enfants surestiment l'intelligence de ces systèmes et les situent dans une catégorie ontologique intermédiaire, entre objet et être vivant \citep{andries2023}. Cette confusion catégorielle les rend sensibles aux indices de surface. Face à un discours fluide et structuré, ils peuvent juger l'information fiable indépendamment de son exactitude factuelle, par effet de fluence de l'instructeur (cf.~\ref{subsec:heuristique_fluidite}). La combinaison d'une tendance anthropomorphique accrue et d'un déficit métacognitif lié à l'âge crée une double vulnérabilité.

Cette vulnérabilité n'est cependant pas absolue. Les enfants ne sont pas des récepteurs passifs d'information. La confrontation directe aux erreurs de l'IA leur permet de calibrer leur confiance et d'adopter une posture plus critique à l'égard des réponses générées. Les collégiennes dont la confiance excessive a été décrite en~\ref{subsec:fiabilite_hallucinations} ont radicalement révisé leur jugement après avoir été exposées aux erreurs de ChatGPT \citep{solyst2024}. Ils mobilisent leurs connaissances pour évaluer la fiabilité de l'agent : ils rejettent les informations incorrectes et testent l'agent avec des questions dont ils connaissent la réponse \citep{oh2025}. Leur confiance est contextuelle. Ils construisent des modèles mentaux distincts selon le type de source, attribuant par exemple une plus grande fiabilité à un moteur de recherche qu'à un humain pour les questions factuelles sur le passé \citep{girouard-hallam2025}. Cette capacité d'évaluation évolue avec l'âge et l'exposition aux technologies. Une transition s'observe au début de l'adolescence : les élèves de 12-13 ans évaluent un chatbot selon son utilité immédiate, tandis que ceux de 14-15 ans le comparent aux standards des assistants grand public \citep{ngyen2022}. La qualité vocale de l'agent module aussi les attributions : une voix artificielle, rapide et prosodiquement pauvre, réduit l'agentivité perçue, même lorsque le contenu est pertinent \citep{xu2025}.

La question qui se pose est celle de la balance entre ces deux dynamiques. D'un côté, les mécanismes de fluence, d'anthropomorphisme et de délégation métacognitive convergent vers une réduction de la vigilance. De l'autre, les apprenants disposent de capacités de calibration qui s'activent sous certaines conditions --- confrontation aux erreurs, mobilisation des connaissances préalables, comparaison entre sources. Les conditions de conception de l'agent déterminent laquelle de ces dynamiques prévaut. La section suivante synthétise les tensions identifiées dans cette revue de littérature et formule les questions de recherche qui en découlent.

