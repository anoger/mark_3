\subsection{Fiabilité et hallucinations : le défi épistémique}
\label{subsec:fiabilite_hallucinations}

Les LLM génèrent du texte en prédisant le token le plus probable étant donné le contexte. Cette génération est intrinsèquement stochastique : le même prompt peut produire des réponses différentes. Le modèle ne dispose pas d'une représentation du monde qu'il consulterait pour vérifier ses énoncés. Il produit des séquences statistiquement plausibles, sans mécanisme interne de validation factuelle. Cette architecture explique le phénomène des hallucinations : la génération d'informations factuellement incorrectes présentées avec une apparente certitude \citep{zhang2025}.

Ces hallucinations peuvent concerner des faits (dates erronées, événements inventés), la fidélité au contexte d'entrée ou la validité logique du raisonnement \citep{zhang2025}. Quelle que soit leur forme, elles partagent une caractéristique commune : le modèle les produit avec la même fluence et la même assurance apparente que ses énoncés corrects. Aucun signal textuel ne distingue le vrai du faux.

Cette absence de marqueurs d'incertitude pose un problème spécifique en contexte pédagogique. L'apprenant ne dispose pas d'indices textuels pour distinguer les énoncés fiables des énoncés erronés. La fluence du discours --- sa fluidité, sa cohérence apparente, son absence d'hésitation --- agit comme un signal implicite de compétence. Les LLM, entraînés à produire un discours naturel et bien structuré, maximisent cette fluence par construction.

La tension entre fluence et exactitude prend une forme particulière en histoire. Le discours historique mobilise des récits, des personnages et des enchaînements causaux qui se prêtent naturellement à la narration. Un LLM peut générer un récit historique parfaitement cohérent sur le plan narratif tout en contenant des erreurs factuelles. La plausibilité narrative masque l'inexactitude factuelle. L'apprenant, confronté à un récit fluide et engageant, peut accepter des informations erronées sans les questionner, précisément parce qu'elles s'insèrent dans une trame cohérente.

Une étude de faisabilité sur la reconstruction de Joseph Lister révèle la nature du problème \citep{dacosta2025}. L'agent, construit avec un système de récupération augmentée, produit des réponses fidèles à la voix historique du personnage. Les évaluations détectent cependant des lacunes dans le cadrage temporel et des embellissements occasionnels. Le modèle situe incorrectement certains événements ou enrichit des détails au-delà de ce que les sources permettent d'affirmer.

Ce cas illustre une difficulté structurelle. Les distorsions ne relèvent pas d'erreurs grossières que l'apprenant pourrait détecter. Elles s'insèrent dans un discours par ailleurs cohérent et engageant. Seul un expert du domaine peut les identifier --- précisément le type de connaissance que l'apprenant novice ne possède pas. La récupération augmentée réduit le risque d'hallucinations flagrantes, mais ne garantit pas l'exactitude des nuances, des interprétations et du cadrage temporel. En histoire, ces nuances constituent souvent l'essentiel de la compréhension disciplinaire.

Le problème dépasse la seule production de contenu erroné. La fluence qui rend le discours des LLM convaincant rend aussi leurs erreurs plus difficiles à détecter. L'écart entre la confiance des utilisateurs et la précision réelle des modèles constitue un problème de calibration. Le mécanisme cognitif qui sous-tend cette vulnérabilité --- la fluence de traitement (\textit{processing fluency}) --- sera examiné dans la section suivante (cf.~\ref{subsec:heuristique_fluidite}), aux côtés de l'illusion de compréhension et de l'autorité perçue de l'agent.

