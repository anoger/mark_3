\subsection{Fiabilité et hallucinations : le défi épistémique}
\label{subsec:fiabilite_hallucinations}

Les LLM génèrent du texte en prédisant le token le plus probable étant donné le contexte. Cette génération est intrinsèquement stochastique : le même prompt peut produire des réponses différentes. Le modèle ne dispose pas d'une représentation du monde qu'il consulterait pour vérifier ses énoncés. Il produit des séquences statistiquement plausibles, sans mécanisme interne de validation factuelle. Cette architecture explique le phénomène des hallucinations : la génération d'informations factuellement incorrectes présentées avec une apparente certitude. Ces hallucinations peuvent concerner des faits (dates erronées, événements inventés), la fidélité au contexte d'entrée ou la validité logique du raisonnement \citep{zhang2025}.

Quelle que soit leur forme, ces hallucinations partagent une caractéristique commune : le modèle les produit avec la même assurance apparente que ses énoncés corrects. Aucun signal textuel --- ni hésitation, ni marqueur d'incertitude, ni avertissement --- ne distingue le vrai du faux \citep{zhang2025}.

Cette absence de marqueurs d'incertitude pose un problème spécifique en contexte pédagogique. Le discours des LLM est structuré, cohérent et exempt d'hésitations, ce qui le rend indiscernable, en surface, d'un discours fiable. Des inexactitudes subtiles présentées avec une apparente autorité constituent ce que la littérature qualifie de « discours négligent » (\textit{negligent discourse}) \citep{wachter2024}. L'apprenant ne dispose d'aucun indice formel pour distinguer les énoncés fiables des énoncés erronés --- et les indices disponibles (structure, cohérence, assurance) sont uniformément rassurants. Une étude auprès de collégiennes confrontées à ChatGPT illustre cette vulnérabilité : 59\,\% des réponses incorrectes sont jugées correctes, une confiance excessive attribuée en partie à la « légitimité esthétique » du discours généré \citep{solyst2024}.

Ce problème prend une forme particulière en histoire. Le discours historique mobilise des récits, des personnages et des enchaînements causaux qui se prêtent naturellement à la narration. Un LLM peut générer un récit historique cohérent sur le plan narratif tout en contenant des erreurs factuelles. La plausibilité narrative tend à masquer l'inexactitude factuelle. Une étude de faisabilité sur la reconstruction conversationnelle de Joseph Lister\footnote{\url{https://fr.wikipedia.org/wiki/Joseph_Lister}}, chirurgien anglais du \textsc{xix}\textsuperscript{e} siècle, en fournit un exemple \citep{dacsta2025}. L'agent, doté d'un système de récupération augmentée, produit des réponses fidèles à la voix historique du personnage, mais les évaluations détectent des lacunes dans le cadrage temporel, des embellissements occasionnels et des événements incorrectement situés. L'apprenant peut accepter ces informations erronées sans les questionner, précisément parce qu'elles s'insèrent dans une trame cohérente.

Ce cas illustre une difficulté structurelle. Les distorsions ne relèvent pas d'erreurs grossières : elles s'insèrent dans un discours par ailleurs cohérent et engageant. Seul un expert du domaine peut les identifier. Ce constat ne se limite pas à l'histoire : dans un contexte moteur, des instructions hallucinées par un LLM ont conduit à des performances significativement inférieures à celles d'un enseignement traditionnel \citep{qiu2024}. La récupération augmentée (RAG, \textit{Retrieval-Augmented Generation}) --- qui ancre les réponses dans des sources vérifiées \citep{lewis2021} --- réduit le risque d'hallucinations flagrantes, mais ne garantit pas l'exactitude des nuances, des interprétations et du cadrage temporel. En histoire, ces nuances constituent souvent l'essentiel de la compréhension disciplinaire.

Le problème dépasse la seule production de contenu erroné. L'absence de signaux d'erreur rend la détection des hallucinations tributaire de connaissances que l'apprenant en situation d'apprentissage ne possède pas encore. Les mécanismes cognitifs qui expliquent pourquoi cette vulnérabilité persiste même face à des erreurs identifiables font l'objet de la section suivante.

