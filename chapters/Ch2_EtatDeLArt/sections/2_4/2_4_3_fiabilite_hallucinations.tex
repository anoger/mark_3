\subsection{Fiabilité et hallucinations : le défi épistémique}
\label{subsec:fiabilite_hallucinations}

Les LLM génèrent du texte en prédisant le token le plus probable étant donné le contexte. Cette génération est intrinsèquement stochastique : le même prompt peut produire des réponses différentes. Le modèle ne dispose pas d'une représentation du monde qu'il consulterait pour vérifier ses énoncés. Il produit des séquences statistiquement plausibles, sans mécanisme interne de validation factuelle. Cette architecture explique le phénomène des hallucinations : la génération d'informations factuellement incorrectes présentées avec une apparente certitude \citep{zhang2025}.

Ces hallucinations peuvent concerner des faits (dates erronées, événements inventés), la fidélité au contexte d'entrée ou la validité logique du raisonnement \citep{zhang2025}. Quelle que soit leur forme, elles partagent une caractéristique commune : le modèle les produit avec la même assurance apparente que ses énoncés corrects. Aucun signal textuel --- ni hésitation, ni marqueur d'incertitude, ni avertissement --- ne distingue le vrai du faux.

Cette absence de marqueurs d'incertitude pose un problème spécifique en contexte pédagogique. L'apprenant ne dispose pas d'indices textuels pour distinguer les énoncés fiables des énoncés erronés. Le discours des LLM est structuré, cohérent et exempt d'hésitations, ce qui le rend indiscernable, en surface, d'un discours fiable. Ce phénomène a été qualifié de « discours négligent » (\textit{negligent discourse}) : des inexactitudes subtiles présentées avec une apparente autorité \citep{wachter2024}. L'apprenant novice, dépourvu des connaissances nécessaires pour évaluer le contenu, ne peut s'appuyer que sur des indices formels --- or ces indices sont uniformément rassurants. Une étude auprès de collégiennes confrontées à ChatGPT montre que 59\,\% des réponses incorrectes sont jugées correctes, une confiance excessive attribuée en partie à la « légitimité esthétique » du discours généré \citep{solyst2024}.

Ce problème prend une forme particulière en histoire. Le discours historique mobilise des récits, des personnages et des enchaînements causaux qui se prêtent naturellement à la narration. Un LLM peut générer un récit historique cohérent sur le plan narratif tout en contenant des erreurs factuelles --- un cas typique d'hallucination en conflit avec les faits, selon la taxonomie de \citet{zhang2025}. La plausibilité narrative tend à masquer l'inexactitude factuelle. Les hallucinations ne sont pas de simples erreurs techniques : elles peuvent avoir un impact mesurable sur l'apprentissage. Dans un contexte moteur, des instructions hallucinées par un LLM (comme « la rotation interne de l'avant-bras ») ont conduit à des performances significativement inférieures à celles d'un enseignement traditionnel \citep{qiu2024}. L'apprenant peut accepter des informations erronées sans les questionner, précisément parce qu'elles s'insèrent dans une trame cohérente.

Une étude de faisabilité sur la reconstruction de Joseph Lister révèle la nature du problème \citep{dacosta2025}. L'agent, construit avec un système de récupération augmentée, produit des réponses fidèles à la voix historique du personnage. Les évaluations détectent cependant des lacunes dans le cadrage temporel et des embellissements occasionnels. Le modèle situe incorrectement certains événements ou enrichit des détails au-delà de ce que les sources permettent d'affirmer.

Ce cas illustre une difficulté structurelle. Les distorsions ne relèvent pas d'erreurs grossières que l'apprenant pourrait détecter. Elles s'insèrent dans un discours par ailleurs cohérent et engageant. Ce type d'inexactitude subtile --- ce que \citet{wachter2024} qualifient de « discours négligent » --- pose un défi spécifique : seul un expert du domaine peut les identifier, précisément le type de connaissance que l'apprenant novice ne possède pas. La récupération augmentée réduit le risque d'hallucinations flagrantes, mais ne garantit pas l'exactitude des nuances, des interprétations et du cadrage temporel. En histoire, ces nuances constituent souvent l'essentiel de la compréhension disciplinaire.

Le problème dépasse la seule production de contenu erroné. L'absence de signaux d'erreur rend la détection des hallucinations dépendante des connaissances préalables de l'utilisateur --- une ressource dont l'apprenant novice dispose par définition en quantité limitée. L'imperceptibilité des erreurs constitue d'ailleurs l'un des défis non résolus identifiés dans les synthèses sur les hallucinations des LLM \citep{zhang2025}. Les mécanismes cognitifs qui expliquent pourquoi cette vulnérabilité persiste même face à des erreurs identifiables font l'objet de la section suivante.

