% Justification des Choix Techniques de MemorIA
% Contenu intégré depuis l'article Computer Animation & Virtual Worlds - 2025 - Oger - MemorIA
% Type [A] - Recopie Technique avec conservation 90-95% du contenu

\subsection{Expressivité émotionnelle : approches d'animation}
\label{subsec:emotional_expressiveness}

La création d'agents conversationnels expressifs, crédibles et réactifs en temps réel pour un usage éducatif implique de relever des défis méthodologiques significatifs. La recherche a principalement exploré les techniques d'animation audio-pilotées (\textit{audio-driven}) et vidéo-pilotées (\textit{video-driven}).

Les approches audio-pilotées visent à générer des animations faciales directement à partir de signaux vocaux. Bien qu'efficaces pour la synchronisation labiale, la capture d'expressions émotionnelles contextuellement appropriées reste difficile. Les méthodes précoces reposaient souvent uniquement sur des caractéristiques acoustiques, résultant en des animations pouvant paraître non naturelles ou déconnectées du contenu sémantique de la parole~\citep{zhangg2022, lu2021}. Des systèmes plus récents exploitent des modèles sophistiqués, tels que des espaces latents unifiés ou des transformateurs de diffusion, montrant des promesses pour une plus grande expressivité~\citep{xu2024, drobyshev2024}. Cependant, ces techniques avancées font souvent face à des limitations pour un déploiement pratique en raison de la complexité computationnelle, d'artefacts visuels potentiels, ou d'un accès restreint (par exemple, étant en code source fermé).

Les techniques vidéo-pilotées excellent généralement dans la production d'animations faciales de haute fidélité et nuancées en transférant le mouvement d'une vidéo source vers un portrait cible~\citep{zhang2022, wang2023, wang2024}. Leur principal inconvénient réside dans l'atteinte de performances temps réel, car les processus d'extraction de mouvement, de reconstruction et de rendu sont souvent computationnellement intensifs, limitant les taux d'images. De plus, l'entraînement de ces modèles peut nécessiter des ressources substantielles (GPUs haut de gamme, larges ensembles de données, temps significatif), et beaucoup sont soumis à des licences propriétaires, entravant leur accessibilité pour une application éducative plus large.

En considérant ces facteurs, le modèle de mouvement du premier ordre (FOMM)~\citep{siarohin2020} a présenté une alternative viable pour MemorIA. FOMM utilise une approche plus simple basée sur des points clés, calculant des transformations affines locales pour déformer un portrait cible selon les mouvements sources. Cette méthode est moins exigeante computationnellement, permettant des performances temps réel (générant des animations de 256 × 256 pixels dans notre cas) sur du matériel modéré. Sa nature open-source facilite également l'implémentation et l'adaptation. Bien que la sortie visuelle de FOMM puisse être moins détaillée que les méthodes vidéo-pilotées de pointe, sa capacité temps réel représentait un compromis nécessaire et pragmatique pour les contraintes de notre application. Des développements ultérieurs tels que LivePortrait~\citep{guo2024}, démontrant une animation temps réel à résolution plus élevée, ont émergé après notre conception initiale mais représentent des pistes potentielles d'amélioration future.

Étant donné le paysage des approches existantes, le choix de combiner NVIDIA Audio2Face et FOMM~\citep{siarohin2020} pour l'animation faciale apparaît comme un compromis approprié entre performance et expressivité. Contrairement aux techniques audio-pilotées classiques, Audio2Face intègre une analyse du signal audio pour identifier un spectre émotionnel potentiellement plus large à travers ses modules Audio2Emotion et AutoEmotion. Cette fonctionnalité permet à l'animation d'aller au-delà de la simple synchronisation labiale, visant à contextualiser les expressions faciales en fonction du contenu sémantique et émotionnel de la parole du LLM.

En analysant le contenu de la parole et les nuances émotionnelles, le système génère des expressions faciales appropriées qui reflètent des émotions de base telles que la joie, la colère ou la tristesse. Il devient ainsi possible d'établir une connexion émotionnelle avec l'agent. Cette intégration représente l'approche de notre architecture : équilibrer les contraintes techniques avec les exigences éducatives pour atteindre des performances temps réel dans les limites d'implémentation des environnements de classe.
